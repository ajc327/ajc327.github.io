<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#020261"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#020261">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="Deploying Time-MoE on Multiple GPUs for Financial Time Series ForecastingTime-MoE (Time Mixture-of-Experts) is a large-scale foundation model for time series forecasting. It’s the first time series Tr">
<meta property="og:type" content="article">
<meta property="og:title" content="time_moe_invest">
<meta property="og:url" content="http://example.com/2025/04/04/time-moe-invest/index.html">
<meta property="og:site_name" content="Efficient Corner">
<meta property="og:description" content="Deploying Time-MoE on Multiple GPUs for Financial Time Series ForecastingTime-MoE (Time Mixture-of-Experts) is a large-scale foundation model for time series forecasting. It’s the first time series Tr">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-04-04T02:27:50.000Z">
<meta property="article:modified_time" content="2025-04-07T04:21:41.568Z">
<meta property="article:author" content="Andy Cai">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2025/04/04/time-moe-invest/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2025/04/04/time-moe-invest/","path":"2025/04/04/time-moe-invest/","title":"time_moe_invest"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>time_moe_invest | Efficient Corner</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.36/fancybox/fancybox.umd.js" integrity="sha256-hiUEBwFEpLF6DlB8sGXlKo4kPZ46Ui4qGpd0vrVkOm4=" crossorigin="anonymous" defer></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  



  <script src="/js/third-party/fancybox.js" defer></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Efficient Corner</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Condensation of experiences</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-photos"><a href="/Photos/" rel="section"><i class="fa fa-camera fa-fw"></i>photos</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Deploying-Time-MoE-on-Multiple-GPUs-for-Financial-Time-Series-Forecasting"><span class="nav-number">1.</span> <span class="nav-text">Deploying Time-MoE on Multiple GPUs for Financial Time Series Forecasting</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-GPU-Training-Setup-with-torchrun-and-DDP"><span class="nav-number">1.1.</span> <span class="nav-text">Multi-GPU Training Setup with torchrun and DDP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Preparing-the-Financial-Time-Series-Dataset-HDF5"><span class="nav-number">1.2.</span> <span class="nav-text">Preparing the Financial Time Series Dataset (HDF5)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loading-the-Pre-trained-Time-MoE-Model-from-Hugging-Face"><span class="nav-number">1.3.</span> <span class="nav-text">Loading the Pre-trained Time-MoE Model from Hugging Face</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Freezing-Most-Layers-and-Fine-Tuning-the-Last-Few"><span class="nav-number">1.4.</span> <span class="nav-text">Freezing Most Layers and Fine-Tuning the Last Few</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimizer-and-Learning-Rate-Warmup"><span class="nav-number">1.5.</span> <span class="nav-text">Optimizer and Learning Rate Warmup</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-Loop-on-Multiple-GPUs"><span class="nav-number">1.6.</span> <span class="nav-text">Training Loop on Multiple GPUs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Forecasting-with-the-Fine-Tuned-Time-MoE-Model"><span class="nav-number">1.7.</span> <span class="nav-text">Forecasting with the Fine-Tuned Time-MoE Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Forecasting-Daily-Stock-Returns"><span class="nav-number">1.7.1.</span> <span class="nav-text">Forecasting Daily Stock Returns</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Forecasting-Lower-Frequency-Indicators-e-g-Quarterly-Data"><span class="nav-number">1.7.2.</span> <span class="nav-text">Forecasting Lower-Frequency Indicators (e.g. Quarterly Data)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">1.8.</span> <span class="nav-text">Conclusion</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Andy Cai</p>
  <div class="site-description" itemprop="description">A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital lab—a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ajc327" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ajc327" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:andycai517@gmail.com" title="E-Mail → mailto:andycai517@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/04/04/time-moe-invest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital lab—a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="time_moe_invest | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          time_moe_invest
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-04-04 10:27:50" itemprop="dateCreated datePublished" datetime="2025-04-04T10:27:50+08:00">2025-04-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-07 12:21:41" itemprop="dateModified" datetime="2025-04-07T12:21:41+08:00">2025-04-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="Deploying-Time-MoE-on-Multiple-GPUs-for-Financial-Time-Series-Forecasting"><a href="#Deploying-Time-MoE-on-Multiple-GPUs-for-Financial-Time-Series-Forecasting" class="headerlink" title="Deploying Time-MoE on Multiple GPUs for Financial Time Series Forecasting"></a>Deploying Time-MoE on Multiple GPUs for Financial Time Series Forecasting</h1><p><strong>Time-MoE</strong> (Time Mixture-of-Experts) is a large-scale foundation model for time series forecasting. It’s the first time series Transformer to scale up to <strong>2.4 billion parameters</strong>, using a decoder-only Mixture-of-Experts (MoE) architecture. Time-MoE operates autoregressively and can handle long input sequences (contexts) up to 4096 time steps. In this guide, we will fine-tune a pre-trained Time-MoE model on financial data (e.g. stock prices&#x2F;returns and economic indicators across various sectors) and deploy it for multi-GPU training. We will cover how to use <strong><code>torchrun</code></strong> with PyTorch Distributed Data Parallel (DDP) for multi-GPU training, how to prepare an HDF5 dataset of financial time series, how to freeze most of the model’s layers (only training the last few layers), apply a learning rate warmup schedule, and finally use the fine-tuned model to forecast both <strong>daily stock returns</strong> and <strong>lower-frequency financial series</strong> (like quarterly fundamentals or econometric indicators).</p>
<h2 id="Multi-GPU-Training-Setup-with-torchrun-and-DDP"><a href="#Multi-GPU-Training-Setup-with-torchrun-and-DDP" class="headerlink" title="Multi-GPU Training Setup with torchrun and DDP"></a>Multi-GPU Training Setup with <code>torchrun</code> and DDP</h2><p>When training on multiple GPUs, we use PyTorch’s <em>Distributed Data Parallel</em> (DDP) to parallelize training across processes (one per GPU). The <code>torchrun</code> command (which supersedes the older <code>torch.distributed.launch</code> utility) helps launch our training script on each GPU process. <code>torchrun</code> automatically sets environment variables like <strong><code>RANK</code></strong>, <strong><code>WORLD_SIZE</code></strong>, and <strong><code>LOCAL_RANK</code></strong> for each process, so that the code can identify the process’s rank (global ID) and local GPU index. Our training script will use these to initialize DDP.</p>
<p><strong>Key steps for DDP initialization:</strong></p>
<ul>
<li><strong>Initialize the process group:</strong> We call <code>torch.distributed.init_process_group(backend=&quot;nccl&quot;)</code> to initialize communication between processes. We use the <strong>NCCL</strong> backend for efficient GPU communication (for CPU-only training, one could use <code>&quot;gloo&quot;</code> backend).</li>
<li><strong>Set the device:</strong> Each process should use a different GPU. We get the local GPU index via <code>LOCAL_RANK</code> (set by torchrun) and call <code>torch.cuda.set_device(local_rank)</code>.</li>
<li><strong>Wrap the model with <code>DistributedDataParallel</code>:</strong> After creating and moving the model to the GPU, we wrap it as <code>DDP(model, device_ids=[local_rank], output_device=local_rank)</code>. This ensures that gradients are averaged across processes during backpropagation.</li>
</ul>
<p>Below is a code snippet illustrating the DDP setup within a training script (e.g. <code>train_time_moe.py</code>):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup_ddp</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initialize distributed training environment.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Get local rank and world size from torchrun environment variables</span></span><br><span class="line">    local_rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;LOCAL_RANK&quot;</span>])</span><br><span class="line">    world_size = <span class="built_in">int</span>(os.environ[<span class="string">&quot;WORLD_SIZE&quot;</span>])</span><br><span class="line">    <span class="comment"># Set the device for this process</span></span><br><span class="line">    torch.cuda.set_device(local_rank)</span><br><span class="line">    <span class="comment"># Initialize the process group for communication</span></span><br><span class="line">    dist.init_process_group(backend=<span class="string">&quot;nccl&quot;</span>)  <span class="comment"># &#x27;nccl&#x27; for GPUs</span></span><br><span class="line">    <span class="keyword">return</span> local_rank, world_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">local_rank, world_size = setup_ddp()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;DDP initialized on GPU <span class="subst">&#123;local_rank&#125;</span> of <span class="subst">&#123;world_size&#125;</span> total GPUs.&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>In this snippet, <code>torchrun</code> would have launched multiple processes and set the necessary env variables. We use them to configure DDP (the master address&#x2F;port are also handled by torchrun, so we did not need to specify them manually). Each process will report its GPU index. After this setup, each process should proceed with creating the model, data loader, etc., using its assigned GPU.</p>
<p><strong>Launching the training:</strong> To run the script on, say, 4 GPUs, you would execute in the shell:<br>torchrun –nproc_per_node&#x3D;4 train_time_moe.py –arg1 <value> –arg2 <value> …</p>
<p>Here <code>--nproc_per_node=4</code> tells torchrun to spawn 4 processes (one per GPU on the node). The script <code>train_time_moe.py</code> should call the setup code as shown. Each process will then run in parallel, but thanks to DDP, they will synchronize gradients during training.</p>
<h2 id="Preparing-the-Financial-Time-Series-Dataset-HDF5"><a href="#Preparing-the-Financial-Time-Series-Dataset-HDF5" class="headerlink" title="Preparing the Financial Time Series Dataset (HDF5)"></a>Preparing the Financial Time Series Dataset (HDF5)</h2><p>Our financial data (such as stock returns, prices, or economic indicators) is assumed to be stored in <strong>HDF5 format</strong> for efficient reading. We need to load this data into PyTorch for training. Typically, we will create a custom <code>Dataset</code> that reads sequences from the HDF5 file and yields them to the DataLoader.</p>
<p><strong>Data organization:</strong> Let’s assume the HDF5 file contains a dataset of time series sequences, each with a fixed length (equal to <code>context_length + prediction_length</code>). For example, each sequence might consist of <code>context_length</code> past observations followed by <code>prediction_length</code> future values (which the model should learn to predict). We will use those future values as training targets. The dataset might be created by sliding a window over longer series or by sampling segments. For simplicity, we assume each index in the HDF5 dataset is already one training sequence.</p>
<p><strong>Using HDF5 in a Dataset:</strong> We can use the <code>h5py</code> library to read HDF5 files. One thing to be careful about in multi-process training is that each DDP process should open its own file handle (to avoid interference). In our <code>Dataset</code> class, we can open the file in the constructor (each process will instantiate its own dataset object, thereby opening the file separately).</p>
<p>Below is an example <code>Dataset</code> for HDF5 time series data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FinancialTimeseriesDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h5_path, context_length, prediction_length</span>):</span><br><span class="line">        <span class="variable language_">self</span>.h5_path = h5_path</span><br><span class="line">        <span class="variable language_">self</span>.context_length = context_length</span><br><span class="line">        <span class="variable language_">self</span>.pred_length = prediction_length</span><br><span class="line">        <span class="comment"># Open the HDF5 file (in read-only mode)</span></span><br><span class="line">        <span class="variable language_">self</span>.file = h5py.File(h5_path, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">        <span class="comment"># Assume the HDF5 has a dataset named &quot;series&quot; with shape (num_samples, seq_length)</span></span><br><span class="line">        <span class="comment"># where seq_length = context_length + prediction_length</span></span><br><span class="line">        <span class="variable language_">self</span>.data = <span class="variable language_">self</span>.file[<span class="string">&#x27;series&#x27;</span>]</span><br><span class="line">        <span class="comment"># Number of sequences in the dataset</span></span><br><span class="line">        <span class="variable language_">self</span>.num_sequences = <span class="variable language_">self</span>.data.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.num_sequences</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># Retrieve the sequence as a numpy array</span></span><br><span class="line">        seq = <span class="variable language_">self</span>.data[idx]  <span class="comment"># shape: (context_length + pred_length,)</span></span><br><span class="line">        <span class="comment"># Split into context and target parts</span></span><br><span class="line">        context = seq[:<span class="variable language_">self</span>.context_length]</span><br><span class="line">        target = seq[<span class="variable language_">self</span>.context_length:]</span><br><span class="line">        <span class="comment"># Convert to torch tensors (float32)</span></span><br><span class="line">        context_tensor = torch.tensor(context, dtype=torch.float32)</span><br><span class="line">        target_tensor = torch.tensor(target, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> context_tensor, target_tensor</span><br></pre></td></tr></table></figure>

<p>In this dataset, each item is a tuple <code>(context, target)</code>:</p>
<ul>
<li><code>context_tensor</code> contains the past values (e.g. past daily returns or indicator values) that the model will use as input.</li>
<li><code>target_tensor</code> contains the next values that we want the model to predict.</li>
</ul>
<p><strong>Distributed sampling:</strong> When using DDP, it’s important to ensure each GPU process sees a unique subset of data each epoch. PyTorch provides <code>torch.utils.data.DistributedSampler</code> to split the dataset across processes. For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, DistributedSampler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose we have already initialized DDP and have `rank` and `world_size`</span></span><br><span class="line">dataset = FinancialTimeseriesDataset(<span class="string">&#x27;financial_data.h5&#x27;</span>, context_length=<span class="number">60</span>, prediction_length=<span class="number">5</span>)</span><br><span class="line">sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=<span class="literal">True</span>)</span><br><span class="line">train_loader = DataLoader(dataset, batch_size=<span class="number">64</span>, sampler=sampler)</span><br></pre></td></tr></table></figure>

<p>Here, <code>DistributedSampler</code> will partition the dataset into <code>world_size</code> chunks and ensure each process (by its <code>rank</code>) only loads its chunk. We pass this sampler to the DataLoader. We also choose a batch size (e.g. 64 sequences per GPU). The sampler’s <code>shuffle=True</code> ensures we shuffle differently each epoch; remember to call <code>sampler.set_epoch(epoch)</code> at the start of each training epoch to get a new shuffling.</p>
<h2 id="Loading-the-Pre-trained-Time-MoE-Model-from-Hugging-Face"><a href="#Loading-the-Pre-trained-Time-MoE-Model-from-Hugging-Face" class="headerlink" title="Loading the Pre-trained Time-MoE Model from Hugging Face"></a>Loading the Pre-trained Time-MoE Model from Hugging Face</h2><p>Time-MoE has been made available on Hugging Face’s model hub (for example, the 50M or 200M model checkpoints). We will use a Hugging Face utility to load the model weights. Since Time-MoE is a custom architecture, we will use <code>AutoModelForCausalLM.from_pretrained(..., trust_remote_code=True)</code> to allow downloading the model’s code from its repository.</p>
<p><strong>Base model checkpoint:</strong> We start from a pre-trained checkpoint such as <code>&quot;Maple728/TimeMoE-50M&quot;</code> or <code>&quot;Maple728/TimeMoE-200M&quot;</code>. Using a pre-trained base is beneficial as it has learned general time series patterns from massive data, which we can fine-tune to our financial domain.</p>
<p>Below is code to load the Time-MoE model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained Time-MoE model from Hugging Face</span></span><br><span class="line">model_name = <span class="string">&quot;Maple728/TimeMoE-50M&quot;</span>  <span class="comment"># or &quot;Maple728/TimeMoE-200M&quot;</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_name,</span><br><span class="line">    trust_remote_code=<span class="literal">True</span>  <span class="comment"># allow loading custom TimeMoE code</span></span><br><span class="line">)</span><br><span class="line">model.to(torch.device(<span class="string">&quot;cuda&quot;</span>, local_rank))  <span class="comment"># move model to this process&#x27;s GPU</span></span><br><span class="line">model.train()  <span class="comment"># set model to training mode</span></span><br></pre></td></tr></table></figure>

<p>The <code>trust_remote_code=True</code> flag is important — it tells 🤗 Transformers to use the model’s custom code from the repository (Time-MoE isn’t a standard architecture like BERT&#x2F;GPT in the library). This code will instantiate the model architecture and load the downloaded weights. After loading, we move the model to the appropriate GPU (<code>local_rank</code>, obtained earlier from DDP setup).</p>
<p>Now that the model is loaded, we will configure it for fine-tuning.</p>
<h2 id="Freezing-Most-Layers-and-Fine-Tuning-the-Last-Few"><a href="#Freezing-Most-Layers-and-Fine-Tuning-the-Last-Few" class="headerlink" title="Freezing Most Layers and Fine-Tuning the Last Few"></a>Freezing Most Layers and Fine-Tuning the Last Few</h2><p>In many fine-tuning scenarios, especially with limited data, it’s beneficial to <strong>freeze</strong> a large portion of the pre-trained model’s parameters so that they remain fixed, and only train (update) the last few layers. This retains the general learned features in early layers and reduces the number of parameters to optimize.</p>
<p>For Time-MoE, we will freeze all layers except the final few Transformer blocks and the output layer(s). The exact number of layers to unfreeze can be a hyperparameter (e.g., unfreeze the last 1–2 layers). We also keep the output head trainable, since it often needs to adapt to the new data’s scale.</p>
<p>In PyTorch, freezing is done by setting <code>param.requires_grad = False</code> for each parameter you want to freeze. For example, to freeze all parameters in a model except those in the last layer, one could do: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment"># Then unfreeze last layer params:</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.fc.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>which is a typical pattern. We will adapt this idea to Time-MoE’s architecture.</p>
<p>Time-MoE’s architecture consists of an input embedding layer, multiple MoE Transformer layers (let’s say <code>N</code> layers in total), and one or more output (forecast) layers. In the <code>transformers</code> implementation, the loaded model likely has an attribute for the stack of transformer layers. For instance, it could be <code>model.model.layers</code> (if the base model is stored in a sub-module <code>model</code>). We will identify the last few layers and unfreeze them.</p>
<p>Here is how we can freeze layers in code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Freeze all parameters by default</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Unfreeze parameters of the last 2 transformer layers (for example)</span></span><br><span class="line">num_layers = model.model.config.num_hidden_layers  <span class="comment"># total number of transformer layers</span></span><br><span class="line"><span class="comment"># Unfreeze last two layers</span></span><br><span class="line"><span class="keyword">for</span> layer_idx <span class="keyword">in</span> <span class="built_in">range</span>(num_layers - <span class="number">2</span>, num_layers):</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.model.layers[layer_idx].parameters():</span><br><span class="line">        param.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Also unfreeze the output prediction layers (Time-MoE may have multiple output heads)</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">&quot;lm_heads&quot;</span>):</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.lm_heads.parameters():</span><br><span class="line">        param.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>In this code:</p>
<ul>
<li>We first set <code>requires_grad=False</code> for all parameters, freezing everything.</li>
<li>Then we retrieve the number of transformer layers from the model’s config (<code>model.model.config.num_hidden_layers</code>). We iterate over the last 2 layer indices and set those layers’ parameters back to <code>True</code> (unfreeze). You can adjust <code>2</code> to another number to unfreeze more or fewer layers.</li>
<li>Additionally, if the model has output heads stored in <code>model.lm_heads</code> (as the Time-MoE implementation does), we unfreeze those as well. This ensures the model can adapt its final predictions to the new data.</li>
</ul>
<p>After this, only the last two transformer layers and the output heads will train; all earlier layer weights remain fixed. This drastically reduces the number of learnable parameters and helps preserve previously learned patterns.</p>
<h2 id="Optimizer-and-Learning-Rate-Warmup"><a href="#Optimizer-and-Learning-Rate-Warmup" class="headerlink" title="Optimizer and Learning Rate Warmup"></a>Optimizer and Learning Rate Warmup</h2><p>With the correct parameters unfrozen, we set up an optimizer. We’ll use <strong>AdamW</strong> (Adam with weight decay) which is a common choice for transformer models. We must ensure to pass only the parameters that <code>require_grad</code> to the optimizer, so it doesn’t try to update frozen weights. We can easily filter those out using <code>model.parameters()</code> generator.</p>
<p>We also implement a <strong>learning rate warmup</strong> schedule. A warmup schedule starts with a low learning rate and gradually increases it during the initial phase of training, which can lead to more stable convergence for transformer models. After the warmup period, the learning rate can then follow a normal decay schedule.</p>
<p>A common strategy is <strong>linear warmup</strong>: increase the LR linearly from 0 up to the target LR over a certain number of steps (often a percentage of total training steps, e.g. 10%). After that, either keep it constant or decay it. In our setup, we’ll use the Hugging Face utility <code>get_linear_schedule_with_warmup</code> to achieve a warmup + linear decay. To use it, we need to know the total number of training steps (batches) in advance.</p>
<p><strong>Computing training steps:</strong> <code>num_training_steps = num_epochs * (dataset_size / (world_size * batch_size))</code>. Each epoch sees <code>dataset_size</code> samples; with <code>world_size</code> GPUs each processing different samples in parallel, the number of steps per epoch per GPU is <code>dataset_size/world_size ÷ batch_size</code>. (If using DistributedSampler, each GPU sees <code>dataset_size/world_size</code> samples per epoch.) So total steps &#x3D; steps per epoch * epochs.</p>
<p>For example, if we have 10,000 sequences, 4 GPUs, batch size 64, and 5 epochs:</p>
<ul>
<li>Each epoch per GPU: 10,000&#x2F;4 &#x3D; 2500 samples per GPU; with batch 64, that’s ~39 steps per GPU per epoch.</li>
<li>Total steps &#x3D; 39 * 5 &#x3D; 195 steps (approx).</li>
</ul>
<p>We can calculate this precisely and then decide on warmup_steps (e.g. 10% of total steps).</p>
<p>Below is code to set up the optimizer and LR scheduler:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_linear_schedule_with_warmup</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># Collect trainable parameters (unfrozen) for optimizer</span></span><br><span class="line">trainable_params = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">optimizer = torch.optim.AdamW(trainable_params, lr=<span class="number">1e-4</span>, weight_decay=<span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure linear warmup schedule (10% of training steps warmup, then linear decay)</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line"><span class="comment"># total training batches per epoch for *this* process:</span></span><br><span class="line">steps_per_epoch = math.ceil(<span class="built_in">len</span>(dataset) / (world_size * train_loader.batch_size))</span><br><span class="line">total_steps = steps_per_epoch * num_epochs</span><br><span class="line">warmup_steps = <span class="built_in">int</span>(<span class="number">0.1</span> * total_steps)  <span class="comment"># 10% of total steps</span></span><br><span class="line"></span><br><span class="line">scheduler = get_linear_schedule_with_warmup(optimizer, </span><br><span class="line">                                            num_warmup_steps=warmup_steps, </span><br><span class="line">                                            num_training_steps=total_steps)</span><br></pre></td></tr></table></figure>

<p>We chose an initial learning rate of 1e-4 (you may tune this) and a weight decay of 1e-2 for regularization. The scheduler here will linearly increase the LR from 0 to 1e-4 over the first 10% of steps, then linearly decrease it back to 0 over the remaining steps.</p>
<h2 id="Training-Loop-on-Multiple-GPUs"><a href="#Training-Loop-on-Multiple-GPUs" class="headerlink" title="Training Loop on Multiple GPUs"></a>Training Loop on Multiple GPUs</h2><p>Now we put everything together in the training loop. Each GPU process will execute this loop on its subset of data. Thanks to DDP, after each backward pass, gradients on each GPU will be averaged across all GPUs, keeping the model parameters in sync.</p>
<p>Important considerations in the training loop:</p>
<ul>
<li>Set the model to train mode (<code>model.train()</code>), which we did after loading.</li>
<li>Iterate over epochs. If using a DistributedSampler, call <code>sampler.set_epoch(epoch)</code> at each epoch start to reshuffle data consistently across processes.</li>
<li>For each batch: move the data to GPU, perform forward pass, compute loss, do backward pass, and update parameters.</li>
<li>Only one process (typically rank 0) should report metrics or save checkpoints, to avoid duplication.</li>
<li>Use <code>torch.no_grad()</code> or simply avoid grad operations during evaluation&#x2F;prediction phases.</li>
</ul>
<p>Time-MoE’s forward pass can directly compute the forecasting loss if we provide the <code>labels</code> and an appropriate mask. The model expects <code>input_ids</code> as a float tensor of shape (batch, seq_len, 1) or (batch, seq_len) for univariate series. We have <code>context</code> and <code>target</code> for each sample. We can concatenate them to form the full sequence, and use that as both input and labels, but supply a <strong>loss mask</strong> to indicate which parts of the sequence to compute loss on. Specifically, we want the model to predict the target portion using the context portion. The Time-MoE implementation uses a <code>loss_masks</code> tensor to achieve this: it should have 0s for the context timesteps (so no loss is computed on the context, since those are given values) and 1s for the prediction timesteps.</p>
<p>We will construct the <code>input_seq</code> by concatenating context and target, and a <code>loss_mask</code> of the same length where positions corresponding to the target segment are 1. Then we call the model with <code>labels=input_seq</code> and <code>loss_masks=loss_mask</code>. The model will return a loss (and possibly predictions), computed only on the target part.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assume: model, optimizer, scheduler, train_loader, sampler are set up as above.</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="comment"># If using DistributedSampler, set the epoch for shuffling</span></span><br><span class="line">    sampler.set_epoch(epoch)</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (context, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="comment"># Concatenate context and target to form full sequence</span></span><br><span class="line">        <span class="comment"># context shape: [batch, context_length], target: [batch, pred_length]</span></span><br><span class="line">        full_seq = torch.cat([context, target], dim=<span class="number">1</span>).unsqueeze(-<span class="number">1</span>).to(torch.float32).to(local_rank)</span><br><span class="line">        <span class="comment"># Prepare loss mask: 0 for context part, 1 for target part</span></span><br><span class="line">        batch_size = context.size(<span class="number">0</span>)</span><br><span class="line">        seq_len = full_seq.size(<span class="number">1</span>)</span><br><span class="line">        mask = torch.zeros((batch_size, seq_len), dtype=torch.float32, device=local_rank)</span><br><span class="line">        mask[:, -target.size(<span class="number">1</span>):] = <span class="number">1.0</span>  <span class="comment"># last pred_length positions set to 1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Forward pass with labels and loss mask</span></span><br><span class="line">        outputs = model(input_ids=full_seq, labels=full_seq, loss_masks=mask)</span><br><span class="line">        loss = outputs.loss  <span class="comment"># Time-MoE returns the loss when labels are provided</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Backpropagation</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (Optional) print loss periodically on rank 0</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">50</span> == <span class="number">0</span> <span class="keyword">and</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Batch <span class="subst">&#123;batch_idx&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># (Optional) checkpoint saving on rank 0</span></span><br><span class="line">    <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">f&quot;timemoe_finetuned_epoch<span class="subst">&#123;epoch&#125;</span>.pt&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>A few notes on the above training procedure:</p>
<ul>
<li>We use <code>full_seq = torch.cat([context, target], dim=1).unsqueeze(-1)</code> to create a tensor of shape <code>[batch, seq_len, 1]</code>. The concatenation ensures the model sees the context followed by the target values. However, due to the causal mask in the model, when predicting each time step it can only use previous time steps.</li>
<li>We create <code>mask</code> of shape <code>[batch, seq_len]</code> (the model code will unsqueeze it internally if needed). The mask has <code>0</code> for all context indices and <code>1</code> for indices corresponding to the future target. This tells the model’s loss function to only compute error on the target part.</li>
<li>We pass <code>labels=full_seq</code> as well. The model will compare its predictions to these labels wherever <code>loss_masks</code> is 1. Essentially, it’s trying to predict the target values (which are included in <code>full_seq</code> but should not be used as input due to the causal mask) from the preceding context.</li>
<li>The loss is obtained from <code>outputs.loss</code>. We call <code>loss.backward()</code> to compute gradients (DDP will handle synchronizing these across GPUs), then update parameters with <code>optimizer.step()</code> and adjust the learning rate with <code>scheduler.step()</code>.</li>
<li>We zero out gradients at the start of each iteration (<code>optimizer.zero_grad()</code>).</li>
<li>Only rank 0 prints the loss and saves checkpoints (to avoid duplicate logs&#x2F;files).</li>
</ul>
<p>During training, DDP will ensure that after each backward pass, the gradients for the unfrozen parameters are averaged across the GPUs. This means each GPU is effectively training on different mini-batches but contributing to a single combined model. The frozen parameters remain unchanged throughout.</p>
<p>After training for the specified epochs, we will have a fine-tuned Time-MoE model specialized on our financial dataset.</p>
<h2 id="Forecasting-with-the-Fine-Tuned-Time-MoE-Model"><a href="#Forecasting-with-the-Fine-Tuned-Time-MoE-Model" class="headerlink" title="Forecasting with the Fine-Tuned Time-MoE Model"></a>Forecasting with the Fine-Tuned Time-MoE Model</h2><p>With a fine-tuned model in hand, we can use it to forecast future values for a given time series. We will demonstrate two scenarios:</p>
<ol>
<li><strong>Daily stock returns forecasting:</strong> high-frequency (daily) predictions for individual stock or index returns.</li>
<li><strong>Lower-frequency indicators forecasting:</strong> e.g., predicting quarterly fundamentals or monthly economic indicators.</li>
</ol>
<p>In both cases, the procedure is similar. The model expects a sequence of past values (normalized if that’s how it was trained) and can generate future values autoregressively. Time-MoE, being a causal model, can generate multiple time steps by iterative prediction (similar to how one would generate multiple words from a language model). The <code>model.generate()</code> function from Hugging Face can be used to roll out predictions for a given <code>max_new_tokens</code> (number of time steps to predict).</p>
<p>Before forecasting, <strong>set the model to evaluation mode</strong> and disable gradient computation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">torch.set_grad_enabled(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>This is important to not accumulate gradients during inference and to apply any dropout in deterministic manner (though dropout might not be present or active in eval mode).</p>
<p>Also, if the model was trained on normalized data, we should normalize the input sequence (and then invert that normalization on the output). In our fine-tuning, if the data was pre-normalized, we may want to apply normalization per series now to match the scale the model saw. A simple approach is to compute the mean and std of the input context and normalize by that, then later rescale predictions.</p>
<h3 id="Forecasting-Daily-Stock-Returns"><a href="#Forecasting-Daily-Stock-Returns" class="headerlink" title="Forecasting Daily Stock Returns"></a>Forecasting Daily Stock Returns</h3><p>For a daily stock returns time series, let’s say we have the last 60 days of returns and we want to forecast the next 5 days. We will prepare the recent 60-day sequence, normalize it (compute mean and std of these 60 values), feed it to the model, and use <code>model.generate</code> to predict 5 additional points.</p>
<p>Example code for daily returns forecasting:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assume we have a 1D array or list `recent_returns` of length 60 (most recent 60 days)</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># Convert to tensor and normalize</span></span><br><span class="line">    recent_seq = torch.tensor(recent_returns, dtype=torch.float32)</span><br><span class="line">    seq_mean = recent_seq.mean()</span><br><span class="line">    seq_std = recent_seq.std() <span class="keyword">if</span> recent_seq.std() &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">1.0</span></span><br><span class="line">    normed_seq = ((recent_seq - seq_mean) / seq_std).unsqueeze(<span class="number">0</span>).unsqueeze(-<span class="number">1</span>).to(local_rank)</span><br><span class="line">    <span class="comment"># Generate 5 future time steps</span></span><br><span class="line">    output = model.generate(normed_seq, max_new_tokens=<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># The output tensor includes the original input plus generated steps</span></span><br><span class="line">    normed_full = output[<span class="number">0</span>, :, <span class="number">0</span>].cpu()  <span class="comment"># shape: (65,) if 60 input + 5 output</span></span><br><span class="line">    normed_predictions = normed_full[-<span class="number">5</span>:]</span><br><span class="line">    <span class="comment"># Inverse normalization to get predictions in original scale</span></span><br><span class="line">    predictions = normed_predictions * seq_std + seq_mean</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Next 5 days predicted returns:&quot;</span>, predictions.numpy())</span><br></pre></td></tr></table></figure>

<p>In this code:</p>
<ul>
<li>We took the <code>recent_returns</code> (an array of shape 60) and made a tensor. We calculated the mean and standard deviation of this sequence.</li>
<li>We normalized the sequence: <code>normed_seq = (recent_seq - mean) / std</code>. This is important if the model was trained on normalized data. Normalization also helps the model handle different scales of series. (If a series has zero variance, we default <code>std</code> to 1 to avoid division by zero.)</li>
<li>We reshape <code>normed_seq</code> to <code>[1, 60, 1]</code> (batch size 1, sequence length 60, feature dim 1) and move it to the same device as the model.</li>
<li>We call <code>model.generate</code> with <code>max_new_tokens=5</code>. Under the hood, this will autoregressively generate 5 new time steps beyond the 60 provided, using the model’s learned dynamics. The result <code>output</code> has shape <code>[1, 60+5, 1]</code>.</li>
<li>We extract the generated part: <code>normed_predictions = output[:, -5:, 0]</code> (taking the last 5 values from the sequence on the 0th batch and 0th feature dimension).</li>
<li>We invert the normalization: multiply by <code>seq_std</code> and add <code>seq_mean</code> to get the predictions back to the original scale of returns.</li>
<li>The result is printed. These would be the model’s forecasts for the next 5 days of returns for that stock.</li>
</ul>
<p>Because Time-MoE was trained on a wide variety of series (and we fine-tuned it on many stocks across sectors), the same model can be used for any particular stock’s forecast. You just feed in that stock’s recent data. In practice, you might loop over many stocks, each time normalizing their recent history and getting predictions.</p>
<h3 id="Forecasting-Lower-Frequency-Indicators-e-g-Quarterly-Data"><a href="#Forecasting-Lower-Frequency-Indicators-e-g-Quarterly-Data" class="headerlink" title="Forecasting Lower-Frequency Indicators (e.g. Quarterly Data)"></a>Forecasting Lower-Frequency Indicators (e.g. Quarterly Data)</h3><p>Forecasting lower-frequency series (like quarterly earnings, monthly macro indicators, etc.) is analogous to the daily case, with the main difference being the frequency of time steps. Suppose we want to forecast the next 4 quarters of an economic indicator given the past 12 quarters of data. The procedure is the same: take the last 12 data points, normalize them, and generate 4 points.</p>
<p>Example for quarterly data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Suppose we have a list `recent_quarters` of length 12 (e.g., last 3 years of quarterly data)</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    recent_seq = torch.tensor(recent_quarters, dtype=torch.float32)</span><br><span class="line">    seq_mean = recent_seq.mean()</span><br><span class="line">    seq_std = recent_seq.std() <span class="keyword">if</span> recent_seq.std() &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">1.0</span></span><br><span class="line">    normed_seq = ((recent_seq - seq_mean) / seq_std).unsqueeze(<span class="number">0</span>).unsqueeze(-<span class="number">1</span>).to(local_rank)</span><br><span class="line">    <span class="comment"># Generate 4 future time steps (e.g., 4 quarters)</span></span><br><span class="line">    output = model.generate(normed_seq, max_new_tokens=<span class="number">4</span>)</span><br><span class="line">    normed_full = output[<span class="number">0</span>, :, <span class="number">0</span>].cpu()</span><br><span class="line">    normed_predictions = normed_full[-<span class="number">4</span>:]</span><br><span class="line">    predictions = normed_predictions * seq_std + seq_mean</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Next 4 quarters predicted values:&quot;</span>, predictions.numpy())</span><br></pre></td></tr></table></figure>

<p>This will output the forecasted values for the next 4 quarters for that indicator&#x2F;series.</p>
<p>A few notes:</p>
<ul>
<li>Even though quarterly data is much lower frequency than daily, Time-MoE can handle it because it treats all sequences as generic time series. The model doesn’t inherently know the calendar frequency; it just learns patterns from the lag structure in the data. If our fine-tuning data included such lower-frequency series, the model will have adapted to them. If not, the model might still produce a reasonable forecast if the series has patterns similar to what it saw.</li>
<li>The context length (12 in this example) and forecast horizon (4) should be chosen based on data availability and the model’s limits. Remember that Time-MoE’s max sequence length is 4096, so we are well within limits.</li>
<li>After obtaining forecasts, you might want to round or post-process them depending on the variable (e.g., ensure no negative values for some indicators if required, etc. — those constraints are outside the model’s direct scope).</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We have shown how to fine-tune and deploy Time-MoE for financial time series forecasting using multiple GPUs. By leveraging <code>torchrun</code> and DDP, we can scale the training across GPUs to speed it up. We stored our large financial dataset in HDF5 for efficient I&#x2F;O and used a custom PyTorch <code>Dataset</code> to feed data into the model. We selectively froze most of the model’s layers, training only the top layers and output heads to specialize the model to our data without overfitting. We also applied a learning rate warmup strategy to stabilize training in the initial phase. </p>
<p>After fine-tuning, our Time-MoE model can generate forecasts for a variety of financial series — from daily stock returns to quarterly indicators — all within a single unified model. We demonstrated how to use the model’s <code>generate</code> method to produce future time steps, and how to normalize and inverse-normalize data around the inference to handle varying scales. This approach allows forecasting across all sectors and stocks with one model, leveraging the cross-sectional patterns learned by Time-MoE.</p>
<p>With this setup, you can further experiment by adjusting the number of layers to unfreeze, tuning the learning rates, changing warmup proportions, or incorporating additional features (covariates) into the dataset if needed. The fine-tuned Time-MoE provides a powerful starting point for building a comprehensive forecasting system in the financial domain.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/LLM/" rel="tag"># LLM</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/03/10/llm-invest/" rel="prev" title="LLM for investment">
                  <i class="fa fa-angle-left"></i> LLM for investment
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Andy Cai</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/ajc327" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>



</body>
</html>
