<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#020261"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#020261">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives,">
<meta property="og:type" content="website">
<meta property="og:title" content="Efficient Corner">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Efficient Corner">
<meta property="og:description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives,">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Andy Cai">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Efficient Corner</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.36/fancybox/fancybox.umd.js" integrity="sha256-hiUEBwFEpLF6DlB8sGXlKo4kPZ46Ui4qGpd0vrVkOm4=" crossorigin="anonymous" defer></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  



  <script src="/js/third-party/fancybox.js" defer></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Efficient Corner</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Condensation of experiences</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-photos"><a href="/photos/" rel="section"><i class="fa fa-camera fa-fw"></i>photos</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Andy Cai</p>
  <div class="site-description" itemprop="description">A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ajc327" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;ajc327" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:andycai517@gmail.com" title="E-Mail â†’ mailto:andycai517@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/04/04/time-moe-invest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/04/04/time-moe-invest/" class="post-title-link" itemprop="url">Time-MoE(Mixture of Experts) for Finance</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-04-04 10:27:50" itemprop="dateCreated datePublished" datetime="2025-04-04T10:27:50+08:00">2025-04-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-07 12:25:38" itemprop="dateModified" datetime="2025-04-07T12:25:38+08:00">2025-04-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Deploying-Time-MoE-on-Multiple-GPUs-for-Financial-Time-Series-Forecasting"><a href="#Deploying-Time-MoE-on-Multiple-GPUs-for-Financial-Time-Series-Forecasting" class="headerlink" title="Deploying Time-MoE on Multiple GPUs for Financial Time Series Forecasting"></a>Deploying Time-MoE on Multiple GPUs for Financial Time Series Forecasting</h1><p><strong>Time-MoE</strong> (Time Mixture-of-Experts) is a large-scale foundation model for time series forecasting. Itâ€™s the first time series Transformer to scale up to <strong>2.4 billion parameters</strong>, using a decoder-only Mixture-of-Experts (MoE) architecture. Time-MoE operates autoregressively and can handle long input sequences (contexts) up to 4096 time steps. In this guide, we will fine-tune a pre-trained Time-MoE model on financial data (e.g. stock prices&#x2F;returns and economic indicators across various sectors) and deploy it for multi-GPU training. We will cover how to use <strong><code>torchrun</code></strong> with PyTorch Distributed Data Parallel (DDP) for multi-GPU training, how to prepare an HDF5 dataset of financial time series, how to freeze most of the modelâ€™s layers (only training the last few layers), apply a learning rate warmup schedule, and finally use the fine-tuned model to forecast both <strong>daily stock returns</strong> and <strong>lower-frequency financial series</strong> (like quarterly fundamentals or econometric indicators).</p>
<h2 id="Multi-GPU-Training-Setup-with-torchrun-and-DDP"><a href="#Multi-GPU-Training-Setup-with-torchrun-and-DDP" class="headerlink" title="Multi-GPU Training Setup with torchrun and DDP"></a>Multi-GPU Training Setup with <code>torchrun</code> and DDP</h2><p>When training on multiple GPUs, we use PyTorchâ€™s <em>Distributed Data Parallel</em> (DDP) to parallelize training across processes (one per GPU). The <code>torchrun</code> command (which supersedes the older <code>torch.distributed.launch</code> utility) helps launch our training script on each GPU process. <code>torchrun</code> automatically sets environment variables like <strong><code>RANK</code></strong>, <strong><code>WORLD_SIZE</code></strong>, and <strong><code>LOCAL_RANK</code></strong> for each process, so that the code can identify the processâ€™s rank (global ID) and local GPU index. Our training script will use these to initialize DDP.</p>
<p><strong>Key steps for DDP initialization:</strong></p>
<ul>
<li><strong>Initialize the process group:</strong> We call <code>torch.distributed.init_process_group(backend=&quot;nccl&quot;)</code> to initialize communication between processes. We use the <strong>NCCL</strong> backend for efficient GPU communication (for CPU-only training, one could use <code>&quot;gloo&quot;</code> backend).</li>
<li><strong>Set the device:</strong> Each process should use a different GPU. We get the local GPU index via <code>LOCAL_RANK</code> (set by torchrun) and call <code>torch.cuda.set_device(local_rank)</code>.</li>
<li><strong>Wrap the model with <code>DistributedDataParallel</code>:</strong> After creating and moving the model to the GPU, we wrap it as <code>DDP(model, device_ids=[local_rank], output_device=local_rank)</code>. This ensures that gradients are averaged across processes during backpropagation.</li>
</ul>
<p>Below is a code snippet illustrating the DDP setup within a training script (e.g. <code>train_time_moe.py</code>):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup_ddp</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initialize distributed training environment.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Get local rank and world size from torchrun environment variables</span></span><br><span class="line">    local_rank = <span class="built_in">int</span>(os.environ[<span class="string">&quot;LOCAL_RANK&quot;</span>])</span><br><span class="line">    world_size = <span class="built_in">int</span>(os.environ[<span class="string">&quot;WORLD_SIZE&quot;</span>])</span><br><span class="line">    <span class="comment"># Set the device for this process</span></span><br><span class="line">    torch.cuda.set_device(local_rank)</span><br><span class="line">    <span class="comment"># Initialize the process group for communication</span></span><br><span class="line">    dist.init_process_group(backend=<span class="string">&quot;nccl&quot;</span>)  <span class="comment"># &#x27;nccl&#x27; for GPUs</span></span><br><span class="line">    <span class="keyword">return</span> local_rank, world_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">local_rank, world_size = setup_ddp()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;DDP initialized on GPU <span class="subst">&#123;local_rank&#125;</span> of <span class="subst">&#123;world_size&#125;</span> total GPUs.&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>In this snippet, <code>torchrun</code> would have launched multiple processes and set the necessary env variables. We use them to configure DDP (the master address&#x2F;port are also handled by torchrun, so we did not need to specify them manually). Each process will report its GPU index. After this setup, each process should proceed with creating the model, data loader, etc., using its assigned GPU.</p>
<p><strong>Launching the training:</strong> To run the script on, say, 4 GPUs, you would execute in the shell:<br>torchrun â€“nproc_per_node&#x3D;4 train_time_moe.py â€“arg1 <value> â€“arg2 <value> â€¦</p>
<p>Here <code>--nproc_per_node=4</code> tells torchrun to spawn 4 processes (one per GPU on the node). The script <code>train_time_moe.py</code> should call the setup code as shown. Each process will then run in parallel, but thanks to DDP, they will synchronize gradients during training.</p>
<h2 id="Preparing-the-Financial-Time-Series-Dataset-HDF5"><a href="#Preparing-the-Financial-Time-Series-Dataset-HDF5" class="headerlink" title="Preparing the Financial Time Series Dataset (HDF5)"></a>Preparing the Financial Time Series Dataset (HDF5)</h2><p>Our financial data (such as stock returns, prices, or economic indicators) is assumed to be stored in <strong>HDF5 format</strong> for efficient reading. We need to load this data into PyTorch for training. Typically, we will create a custom <code>Dataset</code> that reads sequences from the HDF5 file and yields them to the DataLoader.</p>
<p><strong>Data organization:</strong> Letâ€™s assume the HDF5 file contains a dataset of time series sequences, each with a fixed length (equal to <code>context_length + prediction_length</code>). For example, each sequence might consist of <code>context_length</code> past observations followed by <code>prediction_length</code> future values (which the model should learn to predict). We will use those future values as training targets. The dataset might be created by sliding a window over longer series or by sampling segments. For simplicity, we assume each index in the HDF5 dataset is already one training sequence.</p>
<p><strong>Using HDF5 in a Dataset:</strong> We can use the <code>h5py</code> library to read HDF5 files. One thing to be careful about in multi-process training is that each DDP process should open its own file handle (to avoid interference). In our <code>Dataset</code> class, we can open the file in the constructor (each process will instantiate its own dataset object, thereby opening the file separately).</p>
<p>Below is an example <code>Dataset</code> for HDF5 time series data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FinancialTimeseriesDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h5_path, context_length, prediction_length</span>):</span><br><span class="line">        <span class="variable language_">self</span>.h5_path = h5_path</span><br><span class="line">        <span class="variable language_">self</span>.context_length = context_length</span><br><span class="line">        <span class="variable language_">self</span>.pred_length = prediction_length</span><br><span class="line">        <span class="comment"># Open the HDF5 file (in read-only mode)</span></span><br><span class="line">        <span class="variable language_">self</span>.file = h5py.File(h5_path, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">        <span class="comment"># Assume the HDF5 has a dataset named &quot;series&quot; with shape (num_samples, seq_length)</span></span><br><span class="line">        <span class="comment"># where seq_length = context_length + prediction_length</span></span><br><span class="line">        <span class="variable language_">self</span>.data = <span class="variable language_">self</span>.file[<span class="string">&#x27;series&#x27;</span>]</span><br><span class="line">        <span class="comment"># Number of sequences in the dataset</span></span><br><span class="line">        <span class="variable language_">self</span>.num_sequences = <span class="variable language_">self</span>.data.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.num_sequences</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># Retrieve the sequence as a numpy array</span></span><br><span class="line">        seq = <span class="variable language_">self</span>.data[idx]  <span class="comment"># shape: (context_length + pred_length,)</span></span><br><span class="line">        <span class="comment"># Split into context and target parts</span></span><br><span class="line">        context = seq[:<span class="variable language_">self</span>.context_length]</span><br><span class="line">        target = seq[<span class="variable language_">self</span>.context_length:]</span><br><span class="line">        <span class="comment"># Convert to torch tensors (float32)</span></span><br><span class="line">        context_tensor = torch.tensor(context, dtype=torch.float32)</span><br><span class="line">        target_tensor = torch.tensor(target, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> context_tensor, target_tensor</span><br></pre></td></tr></table></figure>

<p>In this dataset, each item is a tuple <code>(context, target)</code>:</p>
<ul>
<li><code>context_tensor</code> contains the past values (e.g. past daily returns or indicator values) that the model will use as input.</li>
<li><code>target_tensor</code> contains the next values that we want the model to predict.</li>
</ul>
<p><strong>Distributed sampling:</strong> When using DDP, itâ€™s important to ensure each GPU process sees a unique subset of data each epoch. PyTorch provides <code>torch.utils.data.DistributedSampler</code> to split the dataset across processes. For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, DistributedSampler</span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose we have already initialized DDP and have `rank` and `world_size`</span></span><br><span class="line">dataset = FinancialTimeseriesDataset(<span class="string">&#x27;financial_data.h5&#x27;</span>, context_length=<span class="number">60</span>, prediction_length=<span class="number">5</span>)</span><br><span class="line">sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=<span class="literal">True</span>)</span><br><span class="line">train_loader = DataLoader(dataset, batch_size=<span class="number">64</span>, sampler=sampler)</span><br></pre></td></tr></table></figure>

<p>Here, <code>DistributedSampler</code> will partition the dataset into <code>world_size</code> chunks and ensure each process (by its <code>rank</code>) only loads its chunk. We pass this sampler to the DataLoader. We also choose a batch size (e.g. 64 sequences per GPU). The samplerâ€™s <code>shuffle=True</code> ensures we shuffle differently each epoch; remember to call <code>sampler.set_epoch(epoch)</code> at the start of each training epoch to get a new shuffling.</p>
<h2 id="Loading-the-Pre-trained-Time-MoE-Model-from-Hugging-Face"><a href="#Loading-the-Pre-trained-Time-MoE-Model-from-Hugging-Face" class="headerlink" title="Loading the Pre-trained Time-MoE Model from Hugging Face"></a>Loading the Pre-trained Time-MoE Model from Hugging Face</h2><p>Time-MoE has been made available on Hugging Faceâ€™s model hub (for example, the 50M or 200M model checkpoints). We will use a Hugging Face utility to load the model weights. Since Time-MoE is a custom architecture, we will use <code>AutoModelForCausalLM.from_pretrained(..., trust_remote_code=True)</code> to allow downloading the modelâ€™s code from its repository.</p>
<p><strong>Base model checkpoint:</strong> We start from a pre-trained checkpoint such as <code>&quot;Maple728/TimeMoE-50M&quot;</code> or <code>&quot;Maple728/TimeMoE-200M&quot;</code>. Using a pre-trained base is beneficial as it has learned general time series patterns from massive data, which we can fine-tune to our financial domain.</p>
<p>Below is code to load the Time-MoE model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained Time-MoE model from Hugging Face</span></span><br><span class="line">model_name = <span class="string">&quot;Maple728/TimeMoE-50M&quot;</span>  <span class="comment"># or &quot;Maple728/TimeMoE-200M&quot;</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_name,</span><br><span class="line">    trust_remote_code=<span class="literal">True</span>  <span class="comment"># allow loading custom TimeMoE code</span></span><br><span class="line">)</span><br><span class="line">model.to(torch.device(<span class="string">&quot;cuda&quot;</span>, local_rank))  <span class="comment"># move model to this process&#x27;s GPU</span></span><br><span class="line">model.train()  <span class="comment"># set model to training mode</span></span><br></pre></td></tr></table></figure>

<p>The <code>trust_remote_code=True</code> flag is important â€” it tells ðŸ¤— Transformers to use the modelâ€™s custom code from the repository (Time-MoE isnâ€™t a standard architecture like BERT&#x2F;GPT in the library). This code will instantiate the model architecture and load the downloaded weights. After loading, we move the model to the appropriate GPU (<code>local_rank</code>, obtained earlier from DDP setup).</p>
<p>Now that the model is loaded, we will configure it for fine-tuning.</p>
<h2 id="Freezing-Most-Layers-and-Fine-Tuning-the-Last-Few"><a href="#Freezing-Most-Layers-and-Fine-Tuning-the-Last-Few" class="headerlink" title="Freezing Most Layers and Fine-Tuning the Last Few"></a>Freezing Most Layers and Fine-Tuning the Last Few</h2><p>In many fine-tuning scenarios, especially with limited data, itâ€™s beneficial to <strong>freeze</strong> a large portion of the pre-trained modelâ€™s parameters so that they remain fixed, and only train (update) the last few layers. This retains the general learned features in early layers and reduces the number of parameters to optimize.</p>
<p>For Time-MoE, we will freeze all layers except the final few Transformer blocks and the output layer(s). The exact number of layers to unfreeze can be a hyperparameter (e.g., unfreeze the last 1â€“2 layers). We also keep the output head trainable, since it often needs to adapt to the new dataâ€™s scale.</p>
<p>In PyTorch, freezing is done by setting <code>param.requires_grad = False</code> for each parameter you want to freeze. For example, to freeze all parameters in a model except those in the last layer, one could do: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment"># Then unfreeze last layer params:</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.fc.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>which is a typical pattern. We will adapt this idea to Time-MoEâ€™s architecture.</p>
<p>Time-MoEâ€™s architecture consists of an input embedding layer, multiple MoE Transformer layers (letâ€™s say <code>N</code> layers in total), and one or more output (forecast) layers. In the <code>transformers</code> implementation, the loaded model likely has an attribute for the stack of transformer layers. For instance, it could be <code>model.model.layers</code> (if the base model is stored in a sub-module <code>model</code>). We will identify the last few layers and unfreeze them.</p>
<p>Here is how we can freeze layers in code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Freeze all parameters by default</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Unfreeze parameters of the last 2 transformer layers (for example)</span></span><br><span class="line">num_layers = model.model.config.num_hidden_layers  <span class="comment"># total number of transformer layers</span></span><br><span class="line"><span class="comment"># Unfreeze last two layers</span></span><br><span class="line"><span class="keyword">for</span> layer_idx <span class="keyword">in</span> <span class="built_in">range</span>(num_layers - <span class="number">2</span>, num_layers):</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.model.layers[layer_idx].parameters():</span><br><span class="line">        param.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Also unfreeze the output prediction layers (Time-MoE may have multiple output heads)</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">&quot;lm_heads&quot;</span>):</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.lm_heads.parameters():</span><br><span class="line">        param.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>In this code:</p>
<ul>
<li>We first set <code>requires_grad=False</code> for all parameters, freezing everything.</li>
<li>Then we retrieve the number of transformer layers from the modelâ€™s config (<code>model.model.config.num_hidden_layers</code>). We iterate over the last 2 layer indices and set those layersâ€™ parameters back to <code>True</code> (unfreeze). You can adjust <code>2</code> to another number to unfreeze more or fewer layers.</li>
<li>Additionally, if the model has output heads stored in <code>model.lm_heads</code> (as the Time-MoE implementation does), we unfreeze those as well. This ensures the model can adapt its final predictions to the new data.</li>
</ul>
<p>After this, only the last two transformer layers and the output heads will train; all earlier layer weights remain fixed. This drastically reduces the number of learnable parameters and helps preserve previously learned patterns.</p>
<h2 id="Optimizer-and-Learning-Rate-Warmup"><a href="#Optimizer-and-Learning-Rate-Warmup" class="headerlink" title="Optimizer and Learning Rate Warmup"></a>Optimizer and Learning Rate Warmup</h2><p>With the correct parameters unfrozen, we set up an optimizer. Weâ€™ll use <strong>AdamW</strong> (Adam with weight decay) which is a common choice for transformer models. We must ensure to pass only the parameters that <code>require_grad</code> to the optimizer, so it doesnâ€™t try to update frozen weights. We can easily filter those out using <code>model.parameters()</code> generator.</p>
<p>We also implement a <strong>learning rate warmup</strong> schedule. A warmup schedule starts with a low learning rate and gradually increases it during the initial phase of training, which can lead to more stable convergence for transformer models. After the warmup period, the learning rate can then follow a normal decay schedule.</p>
<p>A common strategy is <strong>linear warmup</strong>: increase the LR linearly from 0 up to the target LR over a certain number of steps (often a percentage of total training steps, e.g. 10%). After that, either keep it constant or decay it. In our setup, weâ€™ll use the Hugging Face utility <code>get_linear_schedule_with_warmup</code> to achieve a warmup + linear decay. To use it, we need to know the total number of training steps (batches) in advance.</p>
<p><strong>Computing training steps:</strong> <code>num_training_steps = num_epochs * (dataset_size / (world_size * batch_size))</code>. Each epoch sees <code>dataset_size</code> samples; with <code>world_size</code> GPUs each processing different samples in parallel, the number of steps per epoch per GPU is <code>dataset_size/world_size Ã· batch_size</code>. (If using DistributedSampler, each GPU sees <code>dataset_size/world_size</code> samples per epoch.) So total steps &#x3D; steps per epoch * epochs.</p>
<p>For example, if we have 10,000 sequences, 4 GPUs, batch size 64, and 5 epochs:</p>
<ul>
<li>Each epoch per GPU: 10,000&#x2F;4 &#x3D; 2500 samples per GPU; with batch 64, thatâ€™s ~39 steps per GPU per epoch.</li>
<li>Total steps &#x3D; 39 * 5 &#x3D; 195 steps (approx).</li>
</ul>
<p>We can calculate this precisely and then decide on warmup_steps (e.g. 10% of total steps).</p>
<p>Below is code to set up the optimizer and LR scheduler:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_linear_schedule_with_warmup</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># Collect trainable parameters (unfrozen) for optimizer</span></span><br><span class="line">trainable_params = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">optimizer = torch.optim.AdamW(trainable_params, lr=<span class="number">1e-4</span>, weight_decay=<span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure linear warmup schedule (10% of training steps warmup, then linear decay)</span></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line"><span class="comment"># total training batches per epoch for *this* process:</span></span><br><span class="line">steps_per_epoch = math.ceil(<span class="built_in">len</span>(dataset) / (world_size * train_loader.batch_size))</span><br><span class="line">total_steps = steps_per_epoch * num_epochs</span><br><span class="line">warmup_steps = <span class="built_in">int</span>(<span class="number">0.1</span> * total_steps)  <span class="comment"># 10% of total steps</span></span><br><span class="line"></span><br><span class="line">scheduler = get_linear_schedule_with_warmup(optimizer, </span><br><span class="line">                                            num_warmup_steps=warmup_steps, </span><br><span class="line">                                            num_training_steps=total_steps)</span><br></pre></td></tr></table></figure>

<p>We chose an initial learning rate of 1e-4 (you may tune this) and a weight decay of 1e-2 for regularization. The scheduler here will linearly increase the LR from 0 to 1e-4 over the first 10% of steps, then linearly decrease it back to 0 over the remaining steps.</p>
<h2 id="Training-Loop-on-Multiple-GPUs"><a href="#Training-Loop-on-Multiple-GPUs" class="headerlink" title="Training Loop on Multiple GPUs"></a>Training Loop on Multiple GPUs</h2><p>Now we put everything together in the training loop. Each GPU process will execute this loop on its subset of data. Thanks to DDP, after each backward pass, gradients on each GPU will be averaged across all GPUs, keeping the model parameters in sync.</p>
<p>Important considerations in the training loop:</p>
<ul>
<li>Set the model to train mode (<code>model.train()</code>), which we did after loading.</li>
<li>Iterate over epochs. If using a DistributedSampler, call <code>sampler.set_epoch(epoch)</code> at each epoch start to reshuffle data consistently across processes.</li>
<li>For each batch: move the data to GPU, perform forward pass, compute loss, do backward pass, and update parameters.</li>
<li>Only one process (typically rank 0) should report metrics or save checkpoints, to avoid duplication.</li>
<li>Use <code>torch.no_grad()</code> or simply avoid grad operations during evaluation&#x2F;prediction phases.</li>
</ul>
<p>Time-MoEâ€™s forward pass can directly compute the forecasting loss if we provide the <code>labels</code> and an appropriate mask. The model expects <code>input_ids</code> as a float tensor of shape (batch, seq_len, 1) or (batch, seq_len) for univariate series. We have <code>context</code> and <code>target</code> for each sample. We can concatenate them to form the full sequence, and use that as both input and labels, but supply a <strong>loss mask</strong> to indicate which parts of the sequence to compute loss on. Specifically, we want the model to predict the target portion using the context portion. The Time-MoE implementation uses a <code>loss_masks</code> tensor to achieve this: it should have 0s for the context timesteps (so no loss is computed on the context, since those are given values) and 1s for the prediction timesteps.</p>
<p>We will construct the <code>input_seq</code> by concatenating context and target, and a <code>loss_mask</code> of the same length where positions corresponding to the target segment are 1. Then we call the model with <code>labels=input_seq</code> and <code>loss_masks=loss_mask</code>. The model will return a loss (and possibly predictions), computed only on the target part.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assume: model, optimizer, scheduler, train_loader, sampler are set up as above.</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="comment"># If using DistributedSampler, set the epoch for shuffling</span></span><br><span class="line">    sampler.set_epoch(epoch)</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (context, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="comment"># Concatenate context and target to form full sequence</span></span><br><span class="line">        <span class="comment"># context shape: [batch, context_length], target: [batch, pred_length]</span></span><br><span class="line">        full_seq = torch.cat([context, target], dim=<span class="number">1</span>).unsqueeze(-<span class="number">1</span>).to(torch.float32).to(local_rank)</span><br><span class="line">        <span class="comment"># Prepare loss mask: 0 for context part, 1 for target part</span></span><br><span class="line">        batch_size = context.size(<span class="number">0</span>)</span><br><span class="line">        seq_len = full_seq.size(<span class="number">1</span>)</span><br><span class="line">        mask = torch.zeros((batch_size, seq_len), dtype=torch.float32, device=local_rank)</span><br><span class="line">        mask[:, -target.size(<span class="number">1</span>):] = <span class="number">1.0</span>  <span class="comment"># last pred_length positions set to 1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Forward pass with labels and loss mask</span></span><br><span class="line">        outputs = model(input_ids=full_seq, labels=full_seq, loss_masks=mask)</span><br><span class="line">        loss = outputs.loss  <span class="comment"># Time-MoE returns the loss when labels are provided</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Backpropagation</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (Optional) print loss periodically on rank 0</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">50</span> == <span class="number">0</span> <span class="keyword">and</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Batch <span class="subst">&#123;batch_idx&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># (Optional) checkpoint saving on rank 0</span></span><br><span class="line">    <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">f&quot;timemoe_finetuned_epoch<span class="subst">&#123;epoch&#125;</span>.pt&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>A few notes on the above training procedure:</p>
<ul>
<li>We use <code>full_seq = torch.cat([context, target], dim=1).unsqueeze(-1)</code> to create a tensor of shape <code>[batch, seq_len, 1]</code>. The concatenation ensures the model sees the context followed by the target values. However, due to the causal mask in the model, when predicting each time step it can only use previous time steps.</li>
<li>We create <code>mask</code> of shape <code>[batch, seq_len]</code> (the model code will unsqueeze it internally if needed). The mask has <code>0</code> for all context indices and <code>1</code> for indices corresponding to the future target. This tells the modelâ€™s loss function to only compute error on the target part.</li>
<li>We pass <code>labels=full_seq</code> as well. The model will compare its predictions to these labels wherever <code>loss_masks</code> is 1. Essentially, itâ€™s trying to predict the target values (which are included in <code>full_seq</code> but should not be used as input due to the causal mask) from the preceding context.</li>
<li>The loss is obtained from <code>outputs.loss</code>. We call <code>loss.backward()</code> to compute gradients (DDP will handle synchronizing these across GPUs), then update parameters with <code>optimizer.step()</code> and adjust the learning rate with <code>scheduler.step()</code>.</li>
<li>We zero out gradients at the start of each iteration (<code>optimizer.zero_grad()</code>).</li>
<li>Only rank 0 prints the loss and saves checkpoints (to avoid duplicate logs&#x2F;files).</li>
</ul>
<p>During training, DDP will ensure that after each backward pass, the gradients for the unfrozen parameters are averaged across the GPUs. This means each GPU is effectively training on different mini-batches but contributing to a single combined model. The frozen parameters remain unchanged throughout.</p>
<p>After training for the specified epochs, we will have a fine-tuned Time-MoE model specialized on our financial dataset.</p>
<h2 id="Forecasting-with-the-Fine-Tuned-Time-MoE-Model"><a href="#Forecasting-with-the-Fine-Tuned-Time-MoE-Model" class="headerlink" title="Forecasting with the Fine-Tuned Time-MoE Model"></a>Forecasting with the Fine-Tuned Time-MoE Model</h2><p>With a fine-tuned model in hand, we can use it to forecast future values for a given time series. We will demonstrate two scenarios:</p>
<ol>
<li><strong>Daily stock returns forecasting:</strong> high-frequency (daily) predictions for individual stock or index returns.</li>
<li><strong>Lower-frequency indicators forecasting:</strong> e.g., predicting quarterly fundamentals or monthly economic indicators.</li>
</ol>
<p>In both cases, the procedure is similar. The model expects a sequence of past values (normalized if thatâ€™s how it was trained) and can generate future values autoregressively. Time-MoE, being a causal model, can generate multiple time steps by iterative prediction (similar to how one would generate multiple words from a language model). The <code>model.generate()</code> function from Hugging Face can be used to roll out predictions for a given <code>max_new_tokens</code> (number of time steps to predict).</p>
<p>Before forecasting, <strong>set the model to evaluation mode</strong> and disable gradient computation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">torch.set_grad_enabled(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>This is important to not accumulate gradients during inference and to apply any dropout in deterministic manner (though dropout might not be present or active in eval mode).</p>
<p>Also, if the model was trained on normalized data, we should normalize the input sequence (and then invert that normalization on the output). In our fine-tuning, if the data was pre-normalized, we may want to apply normalization per series now to match the scale the model saw. A simple approach is to compute the mean and std of the input context and normalize by that, then later rescale predictions.</p>
<h3 id="Forecasting-Daily-Stock-Returns"><a href="#Forecasting-Daily-Stock-Returns" class="headerlink" title="Forecasting Daily Stock Returns"></a>Forecasting Daily Stock Returns</h3><p>For a daily stock returns time series, letâ€™s say we have the last 60 days of returns and we want to forecast the next 5 days. We will prepare the recent 60-day sequence, normalize it (compute mean and std of these 60 values), feed it to the model, and use <code>model.generate</code> to predict 5 additional points.</p>
<p>Example code for daily returns forecasting:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assume we have a 1D array or list `recent_returns` of length 60 (most recent 60 days)</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># Convert to tensor and normalize</span></span><br><span class="line">    recent_seq = torch.tensor(recent_returns, dtype=torch.float32)</span><br><span class="line">    seq_mean = recent_seq.mean()</span><br><span class="line">    seq_std = recent_seq.std() <span class="keyword">if</span> recent_seq.std() &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">1.0</span></span><br><span class="line">    normed_seq = ((recent_seq - seq_mean) / seq_std).unsqueeze(<span class="number">0</span>).unsqueeze(-<span class="number">1</span>).to(local_rank)</span><br><span class="line">    <span class="comment"># Generate 5 future time steps</span></span><br><span class="line">    output = model.generate(normed_seq, max_new_tokens=<span class="number">5</span>)</span><br><span class="line">    <span class="comment"># The output tensor includes the original input plus generated steps</span></span><br><span class="line">    normed_full = output[<span class="number">0</span>, :, <span class="number">0</span>].cpu()  <span class="comment"># shape: (65,) if 60 input + 5 output</span></span><br><span class="line">    normed_predictions = normed_full[-<span class="number">5</span>:]</span><br><span class="line">    <span class="comment"># Inverse normalization to get predictions in original scale</span></span><br><span class="line">    predictions = normed_predictions * seq_std + seq_mean</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Next 5 days predicted returns:&quot;</span>, predictions.numpy())</span><br></pre></td></tr></table></figure>

<p>In this code:</p>
<ul>
<li>We took the <code>recent_returns</code> (an array of shape 60) and made a tensor. We calculated the mean and standard deviation of this sequence.</li>
<li>We normalized the sequence: <code>normed_seq = (recent_seq - mean) / std</code>. This is important if the model was trained on normalized data. Normalization also helps the model handle different scales of series. (If a series has zero variance, we default <code>std</code> to 1 to avoid division by zero.)</li>
<li>We reshape <code>normed_seq</code> to <code>[1, 60, 1]</code> (batch size 1, sequence length 60, feature dim 1) and move it to the same device as the model.</li>
<li>We call <code>model.generate</code> with <code>max_new_tokens=5</code>. Under the hood, this will autoregressively generate 5 new time steps beyond the 60 provided, using the modelâ€™s learned dynamics. The result <code>output</code> has shape <code>[1, 60+5, 1]</code>.</li>
<li>We extract the generated part: <code>normed_predictions = output[:, -5:, 0]</code> (taking the last 5 values from the sequence on the 0th batch and 0th feature dimension).</li>
<li>We invert the normalization: multiply by <code>seq_std</code> and add <code>seq_mean</code> to get the predictions back to the original scale of returns.</li>
<li>The result is printed. These would be the modelâ€™s forecasts for the next 5 days of returns for that stock.</li>
</ul>
<p>Because Time-MoE was trained on a wide variety of series (and we fine-tuned it on many stocks across sectors), the same model can be used for any particular stockâ€™s forecast. You just feed in that stockâ€™s recent data. In practice, you might loop over many stocks, each time normalizing their recent history and getting predictions.</p>
<h3 id="Forecasting-Lower-Frequency-Indicators-e-g-Quarterly-Data"><a href="#Forecasting-Lower-Frequency-Indicators-e-g-Quarterly-Data" class="headerlink" title="Forecasting Lower-Frequency Indicators (e.g. Quarterly Data)"></a>Forecasting Lower-Frequency Indicators (e.g. Quarterly Data)</h3><p>Forecasting lower-frequency series (like quarterly earnings, monthly macro indicators, etc.) is analogous to the daily case, with the main difference being the frequency of time steps. Suppose we want to forecast the next 4 quarters of an economic indicator given the past 12 quarters of data. The procedure is the same: take the last 12 data points, normalize them, and generate 4 points.</p>
<p>Example for quarterly data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Suppose we have a list `recent_quarters` of length 12 (e.g., last 3 years of quarterly data)</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    recent_seq = torch.tensor(recent_quarters, dtype=torch.float32)</span><br><span class="line">    seq_mean = recent_seq.mean()</span><br><span class="line">    seq_std = recent_seq.std() <span class="keyword">if</span> recent_seq.std() &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">1.0</span></span><br><span class="line">    normed_seq = ((recent_seq - seq_mean) / seq_std).unsqueeze(<span class="number">0</span>).unsqueeze(-<span class="number">1</span>).to(local_rank)</span><br><span class="line">    <span class="comment"># Generate 4 future time steps (e.g., 4 quarters)</span></span><br><span class="line">    output = model.generate(normed_seq, max_new_tokens=<span class="number">4</span>)</span><br><span class="line">    normed_full = output[<span class="number">0</span>, :, <span class="number">0</span>].cpu()</span><br><span class="line">    normed_predictions = normed_full[-<span class="number">4</span>:]</span><br><span class="line">    predictions = normed_predictions * seq_std + seq_mean</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Next 4 quarters predicted values:&quot;</span>, predictions.numpy())</span><br></pre></td></tr></table></figure>

<p>This will output the forecasted values for the next 4 quarters for that indicator&#x2F;series.</p>
<p>A few notes:</p>
<ul>
<li>Even though quarterly data is much lower frequency than daily, Time-MoE can handle it because it treats all sequences as generic time series. The model doesnâ€™t inherently know the calendar frequency; it just learns patterns from the lag structure in the data. If our fine-tuning data included such lower-frequency series, the model will have adapted to them. If not, the model might still produce a reasonable forecast if the series has patterns similar to what it saw.</li>
<li>The context length (12 in this example) and forecast horizon (4) should be chosen based on data availability and the modelâ€™s limits. Remember that Time-MoEâ€™s max sequence length is 4096, so we are well within limits.</li>
<li>After obtaining forecasts, you might want to round or post-process them depending on the variable (e.g., ensure no negative values for some indicators if required, etc. â€” those constraints are outside the modelâ€™s direct scope).</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We have shown how to fine-tune and deploy Time-MoE for financial time series forecasting using multiple GPUs. By leveraging <code>torchrun</code> and DDP, we can scale the training across GPUs to speed it up. We stored our large financial dataset in HDF5 for efficient I&#x2F;O and used a custom PyTorch <code>Dataset</code> to feed data into the model. We selectively froze most of the modelâ€™s layers, training only the top layers and output heads to specialize the model to our data without overfitting. We also applied a learning rate warmup strategy to stabilize training in the initial phase. </p>
<p>After fine-tuning, our Time-MoE model can generate forecasts for a variety of financial series â€” from daily stock returns to quarterly indicators â€” all within a single unified model. We demonstrated how to use the modelâ€™s <code>generate</code> method to produce future time steps, and how to normalize and inverse-normalize data around the inference to handle varying scales. This approach allows forecasting across all sectors and stocks with one model, leveraging the cross-sectional patterns learned by Time-MoE.</p>
<p>With this setup, you can further experiment by adjusting the number of layers to unfreeze, tuning the learning rates, changing warmup proportions, or incorporating additional features (covariates) into the dataset if needed. The fine-tuned Time-MoE provides a powerful starting point for building a comprehensive forecasting system in the financial domain.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/03/10/llm-invest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/03/10/llm-invest/" class="post-title-link" itemprop="url">LLM for investment</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-03-10 10:16:16" itemprop="dateCreated datePublished" datetime="2025-03-10T10:16:16+08:00">2025-03-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-04 11:02:26" itemprop="dateModified" datetime="2025-04-04T11:02:26+08:00">2025-04-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="LLM-empowered-Investments"><a href="#LLM-empowered-Investments" class="headerlink" title="LLM empowered Investments"></a>LLM empowered Investments</h1><p>In this article I introduce a way to use LLMs like the QwQ-32B model to create a sector rotation strategy which is able to outperform an index. </p>
<h2 id="Why-LLMs-new-capability-can-capture-new-inefficiencies"><a href="#Why-LLMs-new-capability-can-capture-new-inefficiencies" class="headerlink" title="Why LLMs - new capability can capture new inefficiencies."></a>Why LLMs - new capability can capture new inefficiencies.</h2><p>LLMs like GPT4 and Deepseek are transforming every industry. Its unique ability to process large amount of texual information systematically is especially useful for quantitative hedge funds, who often formulate their strategies on their ability to process large amount of data quickly and making the most informed decision based on the comprehensive insights generated.  LLMs is useful in many tasks in the quantitative field, from day-to-day tasks like writing code to strategy development, when combined with alternative data such as meeting minutes and reports.</p>
<h2 id="Model-and-Deployment"><a href="#Model-and-Deployment" class="headerlink" title="Model and Deployment"></a>Model and Deployment</h2><ul>
<li>Model: <code>QwQ-32B</code> </li>
<li>GPU: <code>4 x Nvidia A100 40G</code> </li>
<li>Inference solution: <code>Vllm Server</code> </li>
<li>Input Data: <code>news, meeting minutes, reports</code>, about 150 pieces a day.</li>
</ul>
<h1 id="LLM-for-investment-workflow"><a href="#LLM-for-investment-workflow" class="headerlink" title="LLM for investment workflow"></a>LLM for investment workflow</h1><h2 id="1-Data-Preparation-Ingestion"><a href="#1-Data-Preparation-Ingestion" class="headerlink" title="1. Data Preparation &amp; Ingestion"></a>1. Data Preparation &amp; Ingestion</h2><h3 id="1-1-News-Text-Preprocessing"><a href="#1-1-News-Text-Preprocessing" class="headerlink" title="1.1 News Text Preprocessing"></a>1.1 News Text Preprocessing</h3><p>Input: Raw news texts (tâ‚, tâ‚‚, â€¦, tâ‚™).<br><img data-src="/images/llm_1.png" alt="News data from financial terminal">  </p>
<ul>
<li>Steps:<ul>
<li>Clean text (remove HTML, special characters).</li>
<li>Split into individual articles (if not already separated).</li>
<li>Store in a structured format (e.g., JSON) with metadata (date, source).</li>
</ul>
</li>
</ul>
<h3 id="1-2-Stock-Data-Preparation"><a href="#1-2-Stock-Data-Preparation" class="headerlink" title="1.2 Stock Data Preparation"></a>1.2 Stock Data Preparation</h3><p>Data Types:</p>
<ul>
<li>Fundamentals: P&#x2F;E ratios, revenue growth, debt-to-equity.</li>
<li>Price Data: Daily returns, volatility, moving averages.</li>
<li>Sector Metadata: Predefined sector classifications (e.g., GICS sectors).</li>
</ul>
<p>Storage:</p>
<ul>
<li>Use a local SQL database for structured data.</li>
<li>Embed fundamentals&#x2F;textual data into vectors using a local embedding model (e.g., SentenceTransformers).</li>
</ul>
<h2 id="2-Entity-Extraction-RAG-Setup"><a href="#2-Entity-Extraction-RAG-Setup" class="headerlink" title="2. Entity Extraction &amp; RAG Setup"></a>2. Entity Extraction &amp; RAG Setup</h2><h3 id="2-1-Sector-Company-Recognition"><a href="#2-1-Sector-Company-Recognition" class="headerlink" title="2.1 Sector&#x2F;Company Recognition"></a>2.1 Sector&#x2F;Company Recognition</h3><p>Agent 1: Entity Extraction LLM</p>
<p>Prompt:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Identify all companies and their associated sectors mentioned in the text below. Return output as JSON:</span><br><span class="line">&#123;</span><br><span class="line">  &quot;companies&quot;: [&#123;&quot;name&quot;: &quot;Apple&quot;, &quot;sector&quot;: &quot;é…’&quot;&#125;],</span><br><span class="line">  &quot;sectors&quot;: [&quot;Consumer&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Output: Structured sector&#x2F;company data per news text.</p>
<h3 id="2-2-Stock-Data-Retrieval-RAG"><a href="#2-2-Stock-Data-Retrieval-RAG" class="headerlink" title="2.2 Stock Data Retrieval (RAG)"></a>2.2 Stock Data Retrieval (RAG)</h3><ul>
<li><p>Vector Database: FAISS&#x2F;Chroma with embeddings of:</p>
<ul>
<li>Company names + sector metadata.</li>
<li>Key fundamentals (e.g., â€œApple P&#x2F;E: 28.5, Revenue Growth: 8%â€ as text).</li>
</ul>
</li>
<li><p>Retrieval:</p>
<ul>
<li>For each identified company&#x2F;sector, retrieve relevant:</li>
<li>Fundamentals (P&#x2F;E, growth rates).</li>
<li>Recent price trends (e.g., 30-day return).</li>
</ul>
</li>
<li><p>Example retrieved context for a news article about Apple:</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[Apple Inc. Fundamentals]</span><br><span class="line">- P/E Ratio: 28.5 (vs. sector avg: 25.3)</span><br><span class="line">- Revenue Growth (YoY): +8%</span><br><span class="line">- 30-Day Price Change: +12.3%</span><br></pre></td></tr></table></figure>

<h2 id="3-Sentiment-Impact-Analysis"><a href="#3-Sentiment-Impact-Analysis" class="headerlink" title="3. Sentiment &amp; Impact Analysis"></a>3. Sentiment &amp; Impact Analysis</h2><p>Agent 2: News Scoring LLM</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Prompt:</span><br><span class="line">Analyze the news text below and contextual stock data. For each sector, provide:</span><br><span class="line">1. Sentiment (-5 to +5).</span><br><span class="line">2. Impact Magnitude (Low/Medium/High).</span><br><span class="line">3. Rationale (1-2 sentences).</span><br><span class="line"></span><br><span class="line">News Text: &#123;text&#125;</span><br><span class="line">Contextual Data: &#123;retrieved_stock_data&#125;</span><br></pre></td></tr></table></figure>

<p>Output (per text):</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;sectors&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;sector&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Information Technology&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;sentiment&quot;</span><span class="punctuation">:</span> <span class="number">4.2</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;impact&quot;</span><span class="punctuation">:</span> <span class="string">&quot;High&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;rationale&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Positive sentiment due to Apple&#x27;s new product launch...&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="4-Aggregation-Ranking"><a href="#4-Aggregation-Ranking" class="headerlink" title="4. Aggregation &amp; Ranking"></a>4. Aggregation &amp; Ranking</h2><h3 id="4-1-Sector-Score-Calculation"><a href="#4-1-Sector-Score-Calculation" class="headerlink" title="4.1 Sector Score Calculation"></a>4.1 Sector Score Calculation</h3><ul>
<li>Aggregate Metrics:<ul>
<li>Average sentiment score per sector.</li>
<li>Weighted impact (e.g., High&#x3D;3, Medium&#x3D;2, Low&#x3D;1).</li>
<li>Recent price momentum (from stock data).</li>
<li>Fundamental score (e.g., P&#x2F;E percentile vs. sector).</li>
</ul>
</li>
</ul>
<h3 id="4-2-Final-Ranking-LLM"><a href="#4-2-Final-Ranking-LLM" class="headerlink" title="4.2 Final Ranking LLM"></a>4.2 Final Ranking LLM</h3><p>Agent 3: Sector Ranking LLM</p>
<ul>
<li>Input:<ul>
<li>Aggregated sector scores.</li>
<li>Sector fundamentals (avg P&#x2F;E, growth).</li>
<li>Price momentum trends.</li>
</ul>
</li>
</ul>
<p>Prompt:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Rank sectors for investment based on the following criteria:</span><br><span class="line">1. News Sentiment (weight: 40%)</span><br><span class="line">2. Price Momentum (weight: 30%)</span><br><span class="line">3. Fundamentals (weight: 30%)</span><br><span class="line"></span><br><span class="line">Data:</span><br><span class="line">- Information Technology: </span><br><span class="line">  Avg Sentiment: +4.1, Momentum: +12%, P/E: 25.3</span><br><span class="line">- Healthcare: </span><br><span class="line">  Avg Sentiment: +2.8, Momentum: +5%, P/E: 20.1</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;rankings&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;sector&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Information Technology&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;rationale&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Strong product cycles...&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;sector&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Healthcare&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;rationale&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Undervalued despite...&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<ol start="5">
<li>Implementation Tools</li>
</ol>
<ul>
<li><p>LLM Orchestration:</p>
<ul>
<li>LlamaIndex for RAG pipelines.</li>
<li>LangChain for agent workflows.</li>
</ul>
</li>
<li><p>Data Storage:</p>
<ul>
<li>SQLite for structured stock data.</li>
</ul>
</li>
</ul>
<ol start="6">
<li>Workflow Automation<br>Using python, we can link all of this together and complete our LLM sector rotation framework. Here is some Pseudocode.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index <span class="keyword">import</span> VectorStoreIndex, SQLDatabase</span><br><span class="line"><span class="keyword">from</span> langchain_community.llms <span class="keyword">import</span> LlamaCpp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Load data</span></span><br><span class="line">news_texts = load_news()</span><br><span class="line">db = SQLDatabase(<span class="string">&quot;stocks.db&quot;</span>)</span><br><span class="line">vector_index = VectorStoreIndex.load(<span class="string">&quot;sector_vectors&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Process each news text</span></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> news_texts:</span><br><span class="line">    <span class="comment"># Extract entities</span></span><br><span class="line">    entities = llm_entity_extract(text)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve relevant data</span></span><br><span class="line">    context = vector_index.retrieve(entities[<span class="string">&quot;companies&quot;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Analyze sentiment</span></span><br><span class="line">    analysis = llm_analyze(text, context)</span><br><span class="line">    results.append(analysis)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Aggregate scores</span></span><br><span class="line">sector_scores = calculate_aggregates(results)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Generate rankings</span></span><br><span class="line">fundamentals = db.query(<span class="string">&quot;SELECT sector, pe_ratio FROM fundamentals&quot;</span>)</span><br><span class="line">ranking = llm_rank(sector_scores, fundamentals)</span><br></pre></td></tr></table></figure>






      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/03/02/institutional-clients/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/03/02/institutional-clients/" class="post-title-link" itemprop="url">Institutional client relations</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-03-02 19:45:02" itemprop="dateCreated datePublished" datetime="2025-03-02T19:45:02+08:00">2025-03-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-04 10:15:31" itemprop="dateModified" datetime="2025-04-04T10:15:31+08:00">2025-04-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Institutional-client"><a href="#Institutional-client" class="headerlink" title="Institutional client"></a>Institutional client</h1><p>institutional clients, are very sophisticated investors with very specialized investment needs. These invesment goals depends largely on their liability structure. These liability structures are often driven by national policies such as pension, social security, insurance and others. These investors are also trusted by the government to manage a large amount of capital. For these reasons, these institutions are some of the most sophisticated investors in the world, and are very careful about their investment decisions. These institutions not only invests in various markets through its internal investment team, but also frequently employ the help of other investment firms, especially for their expertise in their local markets. This cooperation is often in the form of SMA (seperately managed account) mandates. This represents tremondous opportunity for hedge funds as these mandates often starts at hundreds of Millions and because of their liability structure, are very stable. </p>
<h2 id="1-Who-are-they"><a href="#1-Who-are-they" class="headerlink" title="1. Who are they"></a>1. Who are they</h2><h3 id="1-1-Sovereign-Wealth-Funds"><a href="#1-1-Sovereign-Wealth-Funds" class="headerlink" title="1.1 Sovereign Wealth Funds"></a>1.1 Sovereign Wealth Funds</h3><p>Sovereign wealth funds are the most well known institutional investors, and often the ones that have the most capital behind them. The top soverign wealth funds like the Norway Government Pension Fund, Abu Dhabi Investment Authority (ADIA), have over 1 Trillion USD in AUM. </p>
<p>These institutions are often linked to pension, social security and insurance. These companies are able to employ the most talented professionals as they have extremely important responsibilities. </p>
<h4 id="Investment-Style"><a href="#Investment-Style" class="headerlink" title="Investment Style"></a>Investment Style</h4><p>Sovereign Wealth Funds often have national strategic tasks at hand, for example a liability structure or national development. For these special tasks they often have sub-funds to specifically invest in related fields, for example, domestic infrastructure or tech companies. They will also have many different departments for a vastly diversified portfolio, including global private market equity, public market equity, commodities and forex. This is because only global asset management is capable of taking their tremondous AUM and diversifying away all possible risks. For this reason, all kinds of teams can be found within a Sovereign, with all kinds of different investment objectives. </p>
<h4 id="For-hedge-funds"><a href="#For-hedge-funds" class="headerlink" title="For hedge funds"></a>For hedge funds</h4><p>Sovereigns are the most desirable clients for quant hedge funds, although they often have much lower management fees, often less than $0.2%$ per year. However, with their single mandate size often exceeding 300 Million USD, this is a very desirable client. Sovereigns are often interested in either Enhanced Index type strategies or Market Neutral strategies. The Public market Equity team often looks at the Enhanced index strategy, as a good way of allocating into the beta of a particular country and making some extra along the way. The market neutral strategy on the other hand attracts the Alternative Investment Team, who values the ability for local asset managers to have the expertise to capture Alpha specific to that market. </p>
<h4 id="Getting-their-mandate"><a href="#Getting-their-mandate" class="headerlink" title="Getting their mandate"></a>Getting their mandate</h4><p>In terms of difficulty to get a mandate, these institutions are the absolute hardest. They require multi-year track records, institutional-grade infrastructure (risk systems, compliance), and alignment with their strategic goals. Proposals often undergo multiple rounds of due diligence involving both investment and geopolitical considerations. Relationship-building through industry conferences and consultant networks is critical.</p>
<h3 id="1-2-Insurance-and-Pension-Funds"><a href="#1-2-Insurance-and-Pension-Funds" class="headerlink" title="1.2 Insurance and Pension Funds"></a>1.2 Insurance and Pension Funds</h3><p>These companies are similar to sovereigns in scale but operate under stricter regulatory frameworks. Their liabilities are long-term and predictable (e.g., pension payouts, insurance claims), leading to a focus on duration-matched assets and cash flow stability. Examples include Mercer, Perkeso, and Australian Super.</p>
<h4 id="Investment-Style-1"><a href="#Investment-Style-1" class="headerlink" title="Investment Style"></a>Investment Style</h4><p>Insurance and pension funds prioritize liability-driven investing (LDI). Fixed income dominates portfolios, but alternatives like real estate, infrastructure, and private equity are growing allocations. They often use derivatives for hedging interest rate and inflation risks. Pension funds in aging populations (e.g., Japan, Europe) increasingly seek higher-yielding assets to close funding gaps. They will also allocate a part of their portfolio into Equities.</p>
<h4 id="For-hedge-funds-1"><a href="#For-hedge-funds-1" class="headerlink" title="For hedge funds"></a>For hedge funds</h4><p>These clients value strategies that enhance yield without excessive duration or volatility. Pension funds may allocate to long&#x2F;short equity, global macro, or reinsurance-linked strategies. Insurers favor capital-efficient solutions like volatility targeting or structured credit. Both groups prioritize transparency in risk factors and ESG integration.</p>
<h4 id="Getting-their-mandate-1"><a href="#Getting-their-mandate-1" class="headerlink" title="Getting their mandate"></a>Getting their mandate</h4><p>Regulatory constraints dictate eligible strategies. Expect rigorous stress-testing of tail risks and alignment with actuarial assumptions. Third-party consultants (e.g., Mercer, Aon) play a gatekeeper role. Fee negotiations are intense, but long-term mandates (5+ years) are common once trust is established.</p>
<h3 id="1-3-Banks"><a href="#1-3-Banks" class="headerlink" title="1.3 Banks"></a>1.3 Banks</h3><p>Banks act as both allocators and distributors. Private banks (e.g., UBS, JPMorgan) manage HNWI wealth, while investment banks deploy proprietary capital. Central banks (e.g., SNB, BOJ) manage foreign reserves, often through external managers.</p>
<h4 id="For-hedge-funds-2"><a href="#For-hedge-funds-2" class="headerlink" title="For hedge funds"></a>For hedge funds</h4><p>Private banks seek â€œwhite-labelâ€ strategies for client portfolios, favoring liquid, UCITS-compliant funds. Investment banks allocate to niche strategies (e.g., volatility arbitrage, distressed debt) to complement internal desks. Central banks prioritize liquidity and capital preservation, often using FX overlay or treasury management strategies.</p>
<h4 id="Getting-their-mandate-2"><a href="#Getting-their-mandate-2" class="headerlink" title="Getting their mandate"></a>Getting their mandate</h4><p>Private bank mandates require alignment with their client risk profiles and compliance with distribution platforms (e.g., Allfunds, Fundscape). Investment banks demand real-time risk reporting and capacity for co-investment. Central banks prioritize discretion and political neutrality, often selecting managers through closed RFPs.</p>
<hr>
<h3 id="1-4-Others-First-Loss-Etc"><a href="#1-4-Others-First-Loss-Etc" class="headerlink" title="1.4 Others (First Loss Etc.)"></a>1.4 Others (First Loss Etc.)</h3><p>Specialized entities like family offices, endowments (e.g., Yale University), and structured vehicles (e.g., Boothbay, Topwater) use unique mandate structures. First-loss capital providers absorb initial losses in exchange for higher upside.</p>
<h4 id="For-hedge-funds-3"><a href="#For-hedge-funds-3" class="headerlink" title="For hedge funds"></a>For hedge funds</h4><p>These clients seek tailored solutions. Family offices favor direct deals in private markets, while first-loss providers enable leveraged returns for other LPs. Endowments prioritize illiquid, high-conviction strategies (venture capital, natural resources).</p>
<h4 id="Getting-their-mandate-3"><a href="#Getting-their-mandate-3" class="headerlink" title="Getting their mandate"></a>Getting their mandate</h4><p>Family offices require personal relationships and alignment with principalsâ€™ interests. First-loss mandates demand robust legal frameworks to define waterfall structures. Endowments value academic rigor in investment theses and long-term partnerships. However, in terms of due diligence difficulty, they are the easiest.</p>
<h2 id="2-Why-Asset-Managers-want-to-work-with-them"><a href="#2-Why-Asset-Managers-want-to-work-with-them" class="headerlink" title="2. Why Asset Managers want to work with them"></a>2. Why Asset Managers want to work with them</h2><h3 id="2-1-AUM-Growth"><a href="#2-1-AUM-Growth" class="headerlink" title="2.1 AUM Growth"></a>2.1 AUM Growth</h3><p>Institutional capital is sticky and scalable. A single $500M mandate can transform a mid-sized fundâ€™s trajectory. Recurring revenue from management fees (even at lower rates) stabilizes cash flow, enabling talent acquisition and R&amp;D.</p>
<h3 id="2-2-Reputation"><a href="#2-2-Reputation" class="headerlink" title="2.2 Reputation"></a>2.2 Reputation</h3><p>Institutional validation enhances credibility. A sovereign fund allocation signals operational maturity, easing fundraising from other clients. Public disclosures (e.g., SEC 13F filings) act as free marketing.</p>
<h2 id="3-Tips-on-Interacting-with-them"><a href="#3-Tips-on-Interacting-with-them" class="headerlink" title="3. Tips on Interacting with them"></a>3. Tips on Interacting with them</h2><h3 id="3-1-Respect-the-differences"><a href="#3-1-Respect-the-differences" class="headerlink" title="3.1 Respect the differences"></a>3.1 Respect the differences</h3><p>Pension funds think in decades, hedge funds in days. Avoid jargon â€“ speak to their liability metrics (funding ratios, duration gaps). Often the institution are from different cultures, religions and countries, and we have to ensure we respect their culture. When I went to Malaysia to see Khazanah, they offered no water to everyone in the meeting because it was during Ramadan. Never dismiss regulatory constraints as â€œirrational.â€</p>
<h3 id="3-2-Make-it-easy-for-them"><a href="#3-2-Make-it-easy-for-them" class="headerlink" title="3.2 Make it easy for them"></a>3.2 Make it easy for them</h3><p>Provide institutional reporting standards (GIPS-compliant performance, carbon footprint metrics). Offer segregated accounts for tax&#x2F;regulatory needs. Pre-negotiate legal templates (ISDA, CSA) to accelerate onboarding. Often you need to think in their shoes to make processes as smooth and enjoyable for them as possible. Often those who get their mandates are not those with the best performance, but the ones who can offer them the best service and able to establish themselves as dependable parters. </p>
<h3 id="3-3-Tell-the-truth"><a href="#3-3-Tell-the-truth" class="headerlink" title="3.3 Tell the truth"></a>3.3 Tell the truth</h3><p>Underpromise and overdeliver. Disclose drawdown risks upfront â€“ losing 5% on a â€œlow-volatilityâ€ strategy destroys trust. Flag capacity constraints early. With their level of sophistication and attention to detail in their due diligence process, do not expect to be able to hide anything from them, they will ask the right questions and request for information which will become more and more difficult to lie about. </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/05/28/macro-observations/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/05/28/macro-observations/" class="post-title-link" itemprop="url">Macro Observations</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-05-28 21:43:21" itemprop="dateCreated datePublished" datetime="2024-05-28T21:43:21+08:00">2024-05-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-04 10:15:19" itemprop="dateModified" datetime="2025-04-04T10:15:19+08:00">2025-04-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="1-Major-Market-Event-Studies"><a href="#1-Major-Market-Event-Studies" class="headerlink" title="1. Major Market Event Studies"></a>1. Major Market Event Studies</h1><h2 id="1-1-CSI-500"><a href="#1-1-CSI-500" class="headerlink" title="1.1 CSI 500"></a>1.1 CSI 500</h2><ul>
<li><strong>Weighting Method</strong>: Free-float market capitalization adjusted weighting. First calculate free-float ratio (free-float market cap&#x2F;total market cap), then adjust:<ul>
<li>Ratios &lt;0.1: Use directly</li>
<li>Ratios &gt;0.8: Count as 1.0</li>
<li>Others: Ceiling(ratio, 0.1)</li>
<li>Use adjusted ratio to calculate modified free-float market cap for weighting</li>
</ul>
</li>
</ul>
<h1 id="2-Liquidity-Impacts"><a href="#2-Liquidity-Impacts" class="headerlink" title="2. Liquidity Impacts"></a>2. Liquidity Impacts</h1><h2 id="2-1-Large-vs-Small-Cap-Style-Divergence"><a href="#2-1-Large-vs-Small-Cap-Style-Divergence" class="headerlink" title="2.1 Large vs Small Cap Style Divergence"></a>2.1 Large vs Small Cap Style Divergence</h2><p>Theories mainly based on liquidity considerations:</p>
<ol>
<li><p><strong>Foreign Capital as Marginal Price Setter</strong>:</p>
<ul>
<li>Foreign capital acts as leverage for A-shares</li>
<li>Inflows&#x2F;outflows determined by domestic economic cycle + US Treasury yields</li>
<li>Rising 10Y US Treasury yield â†’ A-share deleveraging</li>
<li>Leverage transmission chain: Foreign capital â†’ Mutual funds â†’ Margin trading</li>
<li>Leverage amplifies social financing base effects while increasing risk&#x2F;volatility</li>
</ul>
</li>
<li><p><strong>10Y US Treasury Yield Direction</strong> is the key environmental factor</p>
</li>
<li><p><strong>Declining 10Y Yield Phase</strong>:</p>
<ul>
<li>Foreign ownership ratio factors dominate early phase</li>
</ul>
</li>
<li><p><strong>Rising 10Y Yield Phase</strong>:</p>
<ul>
<li>â€œConcavity factorsâ€ dominate later phase (weakening small-cap clustering&#x2F;speculative themes, value rotation to core assets)</li>
</ul>
</li>
<li><p><strong>Historical Pattern</strong>:</p>
<ul>
<li>Market bottoms often created by foreign selling, tops by foreign buying</li>
<li>Foreign capital holds marginal pricing power (large size, last buyer&#x2F;seller status)</li>
<li>Hedge through global instruments: Long volatility, short A-shares, long commodities</li>
</ul>
</li>
</ol>
<h1 id="3-Central-Bank-Monetary-Policy"><a href="#3-Central-Bank-Monetary-Policy" class="headerlink" title="3. Central Bank Monetary Policy"></a>3. Central Bank Monetary Policy</h1><h2 id="3-1-Interest-Rate-Cut-Framework"><a href="#3-1-Interest-Rate-Cut-Framework" class="headerlink" title="3.1 Interest Rate Cut Framework"></a>3.1 Interest Rate Cut Framework</h2><ul>
<li><p><strong>Standalone Rate Cuts</strong> target OMO (Open Market Operations) rates</p>
<ol>
<li>Neutralize non-economic factors, let market rates reflect economic reality</li>
<li>Funding rate anchor generally stays â‰¥ policy rate (+~10bps)</li>
<li>When deviation accumulates, policy rate follows funding rate anchor downward</li>
</ol>
</li>
<li><p><strong>Chinaâ€™s â€œFollowing Rate Cutsâ€</strong>:</p>
<ul>
<li>Symbolic (funding rate anchor already reflects reality)</li>
<li>Signals economic weakness recognition</li>
</ul>
</li>
<li><p><strong>US â€œProactive Rate Cutsâ€</strong>:</p>
<ul>
<li>Directly set funding rates</li>
<li>Create immediate market impact</li>
</ul>
</li>
<li><p><strong>Substantive vs Nominal Cuts</strong>:</p>
<ul>
<li>Funding rate anchor decline &#x3D; substantive easing</li>
<li>Policy rate decline &#x3D; nominal adjustment</li>
<li>Sequence: Substantive â†’ Nominal</li>
</ul>
</li>
</ul>
<h2 id="3-2-RRR-Rate-Cuts"><a href="#3-2-RRR-Rate-Cuts" class="headerlink" title="3.2 RRR &amp; Rate Cuts"></a>3.2 RRR &amp; Rate Cuts</h2><ul>
<li><strong>Combined Actions</strong> required for significant easing:<ul>
<li>RRR cuts increase base money supply</li>
<li>Post-COVID multiple RRR&#x2F;rate cuts lowered funding costs</li>
<li>â€œFollowing cutsâ€ act as mild support</li>
<li>â€œProactive cutsâ€ follow comprehensive stimulus needs</li>
</ul>
</li>
</ul>
<h2 id="3-3-Treasury-Yields-Liquidity"><a href="#3-3-Treasury-Yields-Liquidity" class="headerlink" title="3.3 Treasury Yields &amp; Liquidity"></a>3.3 Treasury Yields &amp; Liquidity</h2><ul>
<li><p><strong>Price Components</strong>:</p>
<ol>
<li>Existing liquidity (funding rate anchor)</li>
<li>Expected incremental liquidity</li>
</ol>
</li>
<li><p><strong>10Y Treasury Yield &amp; Market Openness</strong>:</p>
<ul>
<li><strong>Closed Market</strong> (when 10Y CGB &lt; US Treasury):<ul>
<li>Domestic monetary policy effective</li>
<li>Policy-driven bull markets possible</li>
</ul>
</li>
<li><strong>Open Market</strong> (when 10Y CGB &gt; US Treasury):<ul>
<li>Valuation anchor tied to US yields</li>
<li>Market cap shrinkage during outflows</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="4-US-Monetary-Policy"><a href="#4-US-Monetary-Policy" class="headerlink" title="4. US Monetary Policy"></a>4. US Monetary Policy</h1><h2 id="4-1-Fed-Rate-Cut-Framework"><a href="#4-1-Fed-Rate-Cut-Framework" class="headerlink" title="4.1 Fed Rate Cut Framework"></a>4.1 Fed Rate Cut Framework</h2><ul>
<li><p><strong>Decision Drivers</strong>:</p>
<ol>
<li>Maintains â€œappropriateè¿Ÿé’æ„Ÿâ€ (delayed response)</li>
<li>Uses stock performance as economic expectation gauge</li>
<li>Targets: Full employment, price stability, growth</li>
</ol>
</li>
<li><p><strong>Unique Features</strong>:</p>
<ul>
<li>US stocks reflect economic expectations</li>
<li>Persistent stock declines force Fed action</li>
<li>Bond market leads rate expectations</li>
</ul>
</li>
</ul>
<h2 id="4-2-US-10Y-vs-2Y-Treasuries"><a href="#4-2-US-10Y-vs-2Y-Treasuries" class="headerlink" title="4.2 US 10Y vs 2Y Treasuries"></a>4.2 US 10Y vs 2Y Treasuries</h2><h3 id="4-2-1-10Y-Treasury"><a href="#4-2-1-10Y-Treasury" class="headerlink" title="4.2.1 10Y Treasury"></a>4.2.1 10Y Treasury</h3><ul>
<li>Reflects three-way USD supply&#x2F;demand:<ol>
<li>Monetary policy (tracked by 2Y yield)</li>
<li>Risk appetite (equity market flows)</li>
<li>Non-US factors (global USD liquidity)</li>
</ol>
</li>
</ul>
<h3 id="4-2-2-2Y-Treasury"><a href="#4-2-2-2Y-Treasury" class="headerlink" title="4.2.2 2Y Treasury"></a>4.2.2 2Y Treasury</h3><ul>
<li><p>Pure monetary policy expectations proxy</p>
</li>
<li><p><strong>Yield Spread Analysis</strong>:</p>
<ul>
<li>Widening spread â†’ Rising risk appetite â†’ S&amp;P 500 bullish</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/01/02/behavioural-finance-factors/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/02/behavioural-finance-factors/" class="post-title-link" itemprop="url">Quant Factors is about Behavioral Finance</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-01-02 21:16:55" itemprop="dateCreated datePublished" datetime="2024-01-02T21:16:55+08:00">2024-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-04 10:15:44" itemprop="dateModified" datetime="2025-04-04T10:15:44+08:00">2025-04-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="1-Market-Phenomena"><a href="#1-Market-Phenomena" class="headerlink" title="1. Market Phenomena"></a>1. Market Phenomena</h1><h2 id="1-1-Irrational-Behaviors"><a href="#1-1-Irrational-Behaviors" class="headerlink" title="1.1 Irrational Behaviors"></a>1.1 Irrational Behaviors</h2><h3 id="1-1-1-Retail-Investor-Herding-Effect"><a href="#1-1-1-Retail-Investor-Herding-Effect" class="headerlink" title="1.1.1 Retail Investor Herding Effect"></a>1.1.1 Retail Investor Herding Effect</h3><ul>
<li>Key is to separate informed convergence trading from truly blind herding behavior</li>
<li>How to define information? What trading behaviors are informed? </li>
<li>Earlier information arrives â†’ smaller order transactions; later information â†’ larger orders. Throughout this period, there are small-order retail investors following trends. If stock value correlates with price, this creates reflexivity.</li>
<li>Herding should exist across frequencies. At micro level, observe short-term herding. Under T+1 system (unsure about feasibility due to inability to exit quickly). Daily frequency herding manifests as: retail investors initially reluctant to buy, but seeing good intraday gains and high volume, decide to â€œget on the bandwagonâ€. Should examine post-surge trading behaviors - if small orders suddenly increase, indicates trend-chasing behavior (retail buying more as prices rise)</li>
<li>Small-order clusters</li>
</ul>
<ol>
<li>Conformity:  </li>
<li>May have rational component (belief in information cascade + strategic choice to follow others) and irrational component (comfort in following)  </li>
<li>Decision-Making: Individuals find it easier&#x2F;less risky to follow others  </li>
<li>Contagion: Can spread rapidly through groups&#x2F;populations (potential exponential shape&#x2F;feedback loop)  </li>
<li>Context Dependent: Behavior varies across environments. E.g., stocks where herding previously generated profits may continue attracting herd behavior</li>
</ol>
<h3 id="1-1-2-Preference-for-Lottery-Like-Payouts"><a href="#1-1-2-Preference-for-Lottery-Like-Payouts" class="headerlink" title="1.1.2 Preference for Lottery-Like Payouts"></a>1.1.2 Preference for Lottery-Like Payouts</h3><ul>
<li>Investors overvalue lottery-like assets when estimating worth. Can use positive skewness and excess kurtosis as proxy variables.  </li>
<li>Example: In horse racing, gamblers prefer bets with &lt;1&#x2F;100 winning probability (average $1 bet returns $0.39)</li>
</ul>
<h3 id="1-1-3-Overconfidence"><a href="#1-1-3-Overconfidence" class="headerlink" title="1.1.3 Overconfidence"></a>1.1.3 Overconfidence</h3><ul>
<li>Frequent overestimation of oneâ€™s skills&#x2F;knowledge&#x2F;ability to predict future. Leads to excessive trading frequency and risk appetite.  </li>
<li>Trading volume as overconfidence proxy. How to separate truly overconfident trades when some participants have insider information?<br>â€“ Can use returns: If current returns are negative with huge volume, may indicate overconfident retail investors trying to â€œbuy the dipâ€</li>
</ul>
<h4 id="Leverage-as-Overconfidence-Manifestation"><a href="#Leverage-as-Overconfidence-Manifestation" class="headerlink" title="Leverage as Overconfidence Manifestation"></a>Leverage as Overconfidence Manifestation</h4><ul>
<li>Margin trading reflects institutional participants. Increased margin trading shows institutional investorsâ€™ rising risk appetite&#x2F;confidence<br>â€“ Calculate non-institutional trading volume by subtracting margin trades from total volume. Combine with cash flow analysis to improve retail trading identification</li>
</ul>
<h4 id="Overconfidence-Manifestations"><a href="#Overconfidence-Manifestations" class="headerlink" title="Overconfidence Manifestations:"></a>Overconfidence Manifestations:</h4><ol>
<li><p>Overestimation of abilities  </p>
<ol>
<li>Success rate<br><img data-src="/images/factors_1a1.PNG"></li>
</ol>
<p><img data-src="/images/factors_1a2.PNG"><br>2. Pattern: Few large gains + many small losses + low success rate<br>3. Heavy trading during low volatility periods<br><img data-src="/images/factors_1a3.PNG"><br><img data-src="/images/factors_1a4.PNG"><br>4. Transactions significantly deviating from normal prices<br><img data-src="/images/factors_1a5.PNG"><br>5. Trade sizes during low liquidity<br><img data-src="/images/factors_1a6.PNG"><br>6. Average holding periods for profitable vs losing positions<br> <img data-src="/images/factors_1a7.PNG"></p>
</li>
<li><p>Illusion of Control  </p>
<ol>
<li><p>Trading volume<br> <img data-src="/images/factors_1a8.PNG"></p>
</li>
<li><p>Position sizes<br> <img data-src="/images/factors_1b1.PNG"></p>
</li>
<li><p>Holding periods<br> <img data-src="/images/factors_1b2.PNG"></p>
</li>
<li><p>Market impact<br> <img data-src="/images/factors_1b3.PNG"></p>
</li>
</ol>
</li>
<li><p>Underestimation of Risk  </p>
<ol>
<li><p>Trading volume relative to account equity  </p>
</li>
<li><p>Leverage ratio (also check if leverage data can calculate equity)<br> <img data-src="/images/factors_1c2.PNG"></p>
</li>
<li><p>Order types: Presence of stop-loss orders across different time&#x2F;boolean conditions<br> <img data-src="/images/factors_1c3.PNG"></p>
</li>
<li><p>Risk-adjusted returns across order types<br> <img data-src="/images/factors_1c4.PNG"></p>
</li>
<li><p>Order sizes relative to market liquidity<br> <img data-src="/images/factors_1c5.PNG"></p>
</li>
<li><p>Spread sensitivity<br> <img data-src="/images/factors_1c6.PNG"></p>
</li>
<li><p>Trade count during high volatility<br> <img data-src="/images/factors_1c7.PNG"></p>
</li>
<li><p>Order count during order book imbalances<br> <img data-src="/images/factors_1c8.PNG"></p>
</li>
</ol>
</li>
<li><p>Excessive Precision  </p>
<ol>
<li>Narrow-range orders (especially in low liquidity&#x2F;narrow spread conditions)<br> <img data-src="/images/factors_1d1.PNG"></li>
<li>Orders placed during low liquidity<br> <img data-src="/images/factors_1d2.PNG"></li>
</ol>
</li>
<li><p>Confirmation Bias  </p>
<ol>
<li>Requires position data. If positions concentrate in certain groups, observe massive net buying after price rises&#x2F;buying pressure&#x2F;new positive news  </li>
<li>Compare performance between confirmation&#x2F;denial periods  </li>
<li>Check if investors avoid trading during denial periods</li>
</ol>
</li>
<li><p>Over-Optimism  </p>
<ol>
<li>Compare holding periods during price declines vs floating profit periods  </li>
<li>Increased buying willingness&#x2F;aggressiveness during price declines  </li>
<li>Heavy buying despite negative news&#x2F;signals</li>
</ol>
</li>
<li><p>Failure to Learn from Mistakes  </p>
<ol>
<li>Continued buying after losses (small&#x2F;large losses)  </li>
<li>Increased order modification frequency after losses</li>
</ol>
</li>
<li><p>Higher Risk Tolerance  </p>
<ol>
<li>Position size vs account equity  </li>
<li>Trading volume during high volatility  </li>
<li>Order sizes in illiquid markets</li>
</ol>
</li>
<li><p>Decision-Making Without Considering Alternatives (e.g., all-in bets)  </p>
<ol>
<li>Success rate  </li>
<li>Lost opportunities (selling too early)  </li>
<li>Proportion of trades executed during low-liquidity periods</li>
</ol>
</li>
<li><p>Resistance to Feedback  </p>
<ol>
<li>Frequency of strategy adjustments based on market feedback  </li>
<li>Count of consecutive loss periods</li>
</ol>
</li>
</ol>
<h3 id="1-1-4-Loss-Aversion"><a href="#1-1-4-Loss-Aversion" class="headerlink" title="1.1.4 Loss Aversion"></a>1.1.4 Loss Aversion</h3><ul>
<li>Pain from losses â‰ˆ 2x pleasure from equivalent gains (Kahneman &amp; Tversky 1979). Leads to:<br>â€“ Premature profit-taking to â€œlock in gainsâ€<br>â€“ Prolonged holding of underwater positions<br>â€“ Reluctance to realize losses (avoid admitting failure)</li>
</ul>
<h4 id="Manifestations"><a href="#Manifestations" class="headerlink" title="Manifestations:"></a>Manifestations:</h4><ol>
<li>Disproportionate loss impact  </li>
<li>Risk aversion during gains vs risk-seeking during losses  </li>
<li>Endowment Effect  </li>
<li>Disposition Effect  </li>
<li>Framing Effects  </li>
<li>Sunk Cost Fallacy</li>
</ol>
<h3 id="1-1-5-Disposition-Effect"><a href="#1-1-5-Disposition-Effect" class="headerlink" title="1.1.5 Disposition Effect"></a>1.1.5 Disposition Effect</h3><h3 id="1-1-6-Anchoring-Effect"><a href="#1-1-6-Anchoring-Effect" class="headerlink" title="1.1.6 Anchoring Effect"></a>1.1.6 Anchoring Effect</h3><ul>
<li>Investors overweight initial anchors (first-seen prices, round numbers, etc.) in subsequent decisions</li>
</ul>
<h4 id="Analysis-Methods"><a href="#Analysis-Methods" class="headerlink" title="Analysis Methods:"></a>Analysis Methods:</h4><ol>
<li>Identify anchor (opening price, first large trade price, etc.)  </li>
<li>Measure deviations from anchor  </li>
<li>Track order modifications after defined events  </li>
<li>Compare stock vs sector&#x2F;market correlations  </li>
<li>Order&#x2F;volume clustering  </li>
<li>Bid-ask spread consistency during market events</li>
</ol>
<h3 id="1-1-7-Mental-Accounting"><a href="#1-1-7-Mental-Accounting" class="headerlink" title="1.1.7 Mental Accounting"></a>1.1.7 Mental Accounting</h3><ul>
<li>Treat money differently based on source&#x2F;purpose rather than fungibility  </li>
<li>Categorize trades by: investment horizon, market conditions, order type (iceberg orders, etc.)  </li>
<li>Large orders may indicate long-term investors</li>
</ul>
<h4 id="Key-Aspects"><a href="#Key-Aspects" class="headerlink" title="Key Aspects:"></a>Key Aspects:</h4><ol>
<li>Emotional attachment to positions  </li>
<li>Source-dependent spending patterns  </li>
<li>Sunk Cost Fallacy:  <ul>
<li>Persistent averaging down  </li>
<li>Loss reluctance  </li>
<li>Volume concentration at lower price levels</li>
</ul>
</li>
<li>Gain&#x2F;Loss Segregation:  <ul>
<li>Profit-taking at historical highs  </li>
<li>Loss retention at support levels</li>
</ul>
</li>
<li>Small trade costs&#x2F;T+1 small-profit exits  </li>
<li>Budgeting heuristics (fixed-amount trades, regular intervals)  </li>
<li>Framing effects on news reactions</li>
</ol>
<h3 id="1-1-8-Availability-Bias"><a href="#1-1-8-Availability-Bias" class="headerlink" title="1.1.8 Availability Bias"></a>1.1.8 Availability Bias</h3><ul>
<li>Estimating probabilities based on ease of recall rather than objective data</li>
</ul>
<h4 id="Drivers"><a href="#Drivers" class="headerlink" title="Drivers:"></a>Drivers:</h4><ol>
<li>Recency effect  </li>
<li>Vividness (dramatic&#x2F;emotional events)  </li>
<li>Personal experiences  </li>
<li>Media amplification  </li>
<li>Overgeneralization  </li>
<li>Skewed risk perception</li>
</ol>
<h3 id="1-1-9-Confirmation-Bias"><a href="#1-1-9-Confirmation-Bias" class="headerlink" title="1.1.9 Confirmation Bias"></a>1.1.9 Confirmation Bias</h3><ul>
<li>Overweighting information confirming existing views</li>
</ul>
<h4 id="Mechanisms"><a href="#Mechanisms" class="headerlink" title="Mechanisms:"></a>Mechanisms:</h4><ol>
<li>Selective memory&#x2F;attention  </li>
<li>Biased interpretation of ambiguous evidence  </li>
<li>Dismissal of contradictory information</li>
</ol>
<h3 id="1-1-10-Prospect-Theory"><a href="#1-1-10-Prospect-Theory" class="headerlink" title="1.1.10 Prospect Theory"></a>1.1.10 Prospect Theory</h3><ul>
<li>Decisions based on perceived value of S-shaped value function (not absolute gains&#x2F;losses)</li>
</ul>
<h4 id="Key-Elements"><a href="#Key-Elements" class="headerlink" title="Key Elements:"></a>Key Elements:</h4><ol>
<li>Diminishing sensitivity  </li>
<li>Certainty effect  </li>
<li>Loss-domain risk seeking  </li>
<li>Gain-domain risk aversion  </li>
<li>Probability weighting (overweight small probabilities)</li>
</ol>
<h3 id="1-1-11-Endowment-Effect"><a href="#1-1-11-Endowment-Effect" class="headerlink" title="1.1.11 Endowment Effect"></a>1.1.11 Endowment Effect</h3><ul>
<li>Overvaluing owned assets â†’ extended holding periods</li>
</ul>
<h3 id="1-1-12-Regret-Aversion"><a href="#1-1-12-Regret-Aversion" class="headerlink" title="1.1.12 Regret Aversion"></a>1.1.12 Regret Aversion</h3><ul>
<li>Selling winners&#x2F;holding losers to avoid regret</li>
</ul>
<h4 id="Traits"><a href="#Traits" class="headerlink" title="Traits:"></a>Traits:</h4><ol>
<li>Excessive caution  </li>
<li>Mistake avoidance  </li>
<li>Conventional wisdom following  </li>
<li>Cash over-allocation</li>
</ol>
<h3 id="1-1-13-Recency-Bias"><a href="#1-1-13-Recency-Bias" class="headerlink" title="1.1.13 Recency Bias"></a>1.1.13 Recency Bias</h3><ul>
<li>Overweighting recent events vs long-term patterns</li>
</ul>
<h3 id="1-1-14-Status-Quo-Bias"><a href="#1-1-14-Status-Quo-Bias" class="headerlink" title="1.1.14 Status Quo Bias"></a>1.1.14 Status Quo Bias</h3><ul>
<li>Preference for existing positions despite new information</li>
</ul>
<h3 id="1-1-15-Emotional-Decision-Making"><a href="#1-1-15-Emotional-Decision-Making" class="headerlink" title="1.1.15 Emotional Decision-Making"></a>1.1.15 Emotional Decision-Making</h3><ul>
<li>Positive emotions â†’ over-optimism  </li>
<li>Negative emotions â†’ risk overestimation</li>
</ul>
<h3 id="1-1-16-Short-Term-Bias"><a href="#1-1-16-Short-Term-Bias" class="headerlink" title="1.1.16 Short-Term Bias"></a>1.1.16 Short-Term Bias</h3><ul>
<li>Preference for immediate small gains over larger delayed rewards</li>
</ul>
<h3 id="1-1-17-Gamblerâ€™s-Fallacy"><a href="#1-1-17-Gamblerâ€™s-Fallacy" class="headerlink" title="1.1.17 Gamblerâ€™s Fallacy"></a>1.1.17 Gamblerâ€™s Fallacy</h3><ul>
<li>â€œDue for win&#x2F;lossâ€ mentality â†’ irrational risk-taking</li>
</ul>
<h3 id="1-1-18-Narrative-Fallacy"><a href="#1-1-18-Narrative-Fallacy" class="headerlink" title="1.1.18 Narrative Fallacy"></a>1.1.18 Narrative Fallacy</h3><ul>
<li>Story-driven investing over data analysis</li>
</ul>
<h3 id="1-1-19-Neglect-of-Base-Rates"><a href="#1-1-19-Neglect-of-Base-Rates" class="headerlink" title="1.1.19 Neglect of Base Rates"></a>1.1.19 Neglect of Base Rates</h3><ul>
<li>Ignoring statistical norms for salient anecdotes</li>
</ul>
<h2 id="1-2-Concession-Behaviors"><a href="#1-2-Concession-Behaviors" class="headerlink" title="1.2 Concession Behaviors"></a>1.2 Concession Behaviors</h2><h3 id="1-2-1-Closing-Price-Manipulation"><a href="#1-2-1-Closing-Price-Manipulation" class="headerlink" title="1.2.1 Closing Price Manipulation"></a>1.2.1 Closing Price Manipulation</h3><ul>
<li>Common in highly leveraged assets</li>
</ul>
<h3 id="1-2-2-Window-Dressing"><a href="#1-2-2-Window-Dressing" class="headerlink" title="1.2.2 Window Dressing"></a>1.2.2 Window Dressing</h3><ul>
<li>Portfolio pumping before reporting dates</li>
</ul>
<h3 id="1-2-3-National-Team-Interventions"><a href="#1-2-3-National-Team-Interventions" class="headerlink" title="1.2.3 National Team Interventions"></a>1.2.3 National Team Interventions</h3><ul>
<li>Participants: Central Huijin, China Securities Finance Corp (CSF), SOEs  </li>
<li>Methods:  <ul>
<li>Direct large-cap&#x2F;ETF purchases  </li>
<li>Broker liquidity support  </li>
<li>Corporate buyback promotion</li>
</ul>
</li>
<li>Factor construction: Isolate government-driven volume</li>
</ul>
<h1 id="2-High-Frequency-â†’-Minute-Level-Processing"><a href="#2-High-Frequency-â†’-Minute-Level-Processing" class="headerlink" title="2. High-Frequency â†’ Minute-Level Processing"></a>2. High-Frequency â†’ Minute-Level Processing</h1><ul>
<li>Features: Slopes, imbalances, microstructure params, basic stats, bid&#x2F;ask flow, order sizes</li>
</ul>
<h1 id="3-Minute-Level-Operations"><a href="#3-Minute-Level-Operations" class="headerlink" title="3. Minute-Level Operations"></a>3. Minute-Level Operations</h1><h2 id="3-1-Regime-Segmentation"><a href="#3-1-Regime-Segmentation" class="headerlink" title="3.1 Regime Segmentation"></a>3.1 Regime Segmentation</h2><ul>
<li>Boolean triggers:  <ol>
<li>Price vs open  </li>
<li>Time periods  </li>
<li>Event windows  </li>
<li>High&#x2F;low price zones  </li>
<li>High volatility&#x2F;volume  </li>
<li>Entropy&#x2F;imbalance extremes  </li>
<li>Liquidity regimes  </li>
<li>Retail&#x2F;institutional P&amp;L states</li>
</ol>
</li>
</ul>
<h1 id="4-Minuteâ†’Daily-Aggregation"><a href="#4-Minuteâ†’Daily-Aggregation" class="headerlink" title="4. Minuteâ†’Daily Aggregation"></a>4. Minuteâ†’Daily Aggregation</h1><h2 id="4-1-Intra-Regime-Feature-Significance"><a href="#4-1-Intra-Regime-Feature-Significance" class="headerlink" title="4.1 Intra-Regime Feature Significance"></a>4.1 Intra-Regime Feature Significance</h2><ul>
<li>Metrics:  <ol>
<li>Cumulative volume&#x2F;VWAP ratio  </li>
<li>Trend ratio (total price change vs cumulative absolute changes)  </li>
<li>Top&#x2F;bottom % price clusters</li>
</ol>
</li>
</ul>
<h2 id="4-2-Boolean-Partitioning"><a href="#4-2-Boolean-Partitioning" class="headerlink" title="4.2 Boolean Partitioning"></a>4.2 Boolean Partitioning</h2><ul>
<li>Boolean ratios (std&#x2F;sum&#x2F;mean of returns)  </li>
<li>Oscillator thresholding</li>
</ul>
<h2 id="4-3-Factor-Concepts"><a href="#4-3-Factor-Concepts" class="headerlink" title="4.3 Factor Concepts"></a>4.3 Factor Concepts</h2><ol>
<li>4-day retail trapped volume  </li>
<li>4-day retail loss unwinding  </li>
<li>4-day retail profit-taking  </li>
<li>Retail â€œsell-too-earlyâ€ magnitude  </li>
<li>Retail â€œcut-lossâ€ intensity  </li>
<li>Retail capitulation vs institutional selling  </li>
<li>Retail loss vs institutional profit  </li>
<li>Retail profit vs institutional exit  </li>
<li>Retail conviction metrics</li>
</ol>
<h1 id="5-Common-Factor-Analysis"><a href="#5-Common-Factor-Analysis" class="headerlink" title="5. Common Factor Analysis"></a>5. Common Factor Analysis</h1><ol>
<li>Return: 5-day effect strongest when orthogonal to industry&#x2F;size  </li>
<li>[Remaining content truncated]</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/08/pyspark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/08/pyspark/" class="post-title-link" itemprop="url">Working with PySpark - Practical Tips</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-08 20:27:59" itemprop="dateCreated datePublished" datetime="2023-07-08T20:27:59+08:00">2023-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-04 10:15:05" itemprop="dateModified" datetime="2025-04-04T10:15:05+08:00">2025-04-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>As a quant PM working with large datasets, PySpark has become an indispensable tool in my data processing toolkit. In this post, Iâ€™ll share some practical insights about working with PySpark components and execution mechanics.</p>
<h2 id="1-PySpark-SQL-DataFrame-Essentials"><a href="#1-PySpark-SQL-DataFrame-Essentials" class="headerlink" title="1. PySpark SQL DataFrame Essentials"></a>1. PySpark SQL DataFrame Essentials</h2><h3 id="Reading-Writing-Data"><a href="#Reading-Writing-Data" class="headerlink" title="Reading&#x2F;Writing Data"></a>Reading&#x2F;Writing Data</h3><p>To persist DataFrames in HDFS (common in enterprise environments):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Write to Hive table</span></span><br><span class="line">df.write.mode(<span class="string">&quot;overwrite&quot;</span>).orc(<span class="string">&quot;/user/hive/warehouse/user_cjt_db.db/test1.orc&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read back</span></span><br><span class="line">df = spark.read.orc(<span class="string">&quot;/user/hive/warehouse/user_cjt_db.db/test1.orc&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>The <code>mode(&quot;overwrite&quot;)</code> flag helps avoid accidental data duplication.</p>
<h3 id="Handling-Nulls-in-Array-Operations"><a href="#Handling-Nulls-in-Array-Operations" class="headerlink" title="Handling Nulls in Array Operations"></a>Handling Nulls in Array Operations</h3><p>When using <code>collect_list</code> for row conversions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.fillna(&#123;<span class="string">&#x27;column_with_null_values&#x27;</span>: <span class="string">&#x27;default_value&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure>

<p>Sparkâ€™s <code>collect_list</code> preserves nulls by default - a common pitfall when preparing features. Always handle nulls first!</p>
<h2 id="2-PySpark-Pandas-API-Nuances"><a href="#2-PySpark-Pandas-API-Nuances" class="headerlink" title="2. PySpark Pandas API Nuances"></a>2. PySpark Pandas API Nuances</h2><p>While the Pandas API provides familiar syntax, watch out for:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Writing with pandas API</span></span><br><span class="line">df.coalesce(<span class="number">1</span>).pandas_api().to_orc(<span class="string">&quot;user_cjt_db/active_buy.orc&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reading</span></span><br><span class="line">pdf = ps.read_orc(<span class="string">&quot;user_cjt_db/active_buy.orc&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Important: pandas_api() creates COPY</span></span><br><span class="line">pdf = df.pandas_api()</span><br><span class="line">pdf[<span class="string">&#x27;new_col&#x27;</span>] = <span class="number">1</span>  <span class="comment"># Original df remains unchanged</span></span><br></pre></td></tr></table></figure>

<p>The pandas API is great for small datasets but be mindful of:</p>
<ul>
<li>Coalescing partitions before write</li>
<li>Implicit data copies</li>
<li>Limited distributed computing benefits</li>
</ul>
<h2 id="3-Understanding-RDDs"><a href="#3-Understanding-RDDs" class="headerlink" title="3. Understanding RDDs"></a>3. Understanding RDDs</h2><p>Resilient Distributed Datasets (RDDs) remain Sparkâ€™s foundation:</p>
<ul>
<li><strong>Distributed</strong>: Data partitioned across cluster nodes</li>
<li><strong>Fault-tolerant</strong>: Automatic recovery from node failures</li>
<li><strong>Lazy execution</strong>: Optimizes operation sequence before execution</li>
</ul>
<p>While DataFrames are now preferred for most use cases, RDDs still power:</p>
<ul>
<li>Custom distributed algorithms</li>
<li>Fine-grained control over partitioning</li>
<li>Legacy system integrations</li>
</ul>
<h2 id="4-Execution-Hierarchy-Jobs-Stages-Tasks"><a href="#4-Execution-Hierarchy-Jobs-Stages-Tasks" class="headerlink" title="4. Execution Hierarchy: Jobs, Stages &amp; Tasks"></a>4. Execution Hierarchy: Jobs, Stages &amp; Tasks</h2><p><img data-src="/images/pyspark_aqe.png" alt="Spark execution hierarchy diagram"></p>
<ol>
<li><p><strong>Jobs</strong>: Triggered by actions (<code>count()</code>, <code>save()</code>, etc.)</p>
<ul>
<li>One job per action</li>
<li>Example: <code>df.write.orc(...)</code> creates a job</li>
</ul>
</li>
<li><p><strong>Stages</strong>: Groups of tasks that can execute together</p>
<ul>
<li>Boundary at shuffle operations</li>
<li>Map stage: Narrow transformations (filter, map)</li>
<li>Reduce stage: Wide transformations (groupBy, join)</li>
</ul>
</li>
<li><p><strong>Tasks</strong>: Smallest execution unit</p>
<ul>
<li>Number of tasks &#x3D; number of partitions</li>
<li>Run on individual executors</li>
</ul>
</li>
</ol>
<h2 id="5-Adaptive-Query-Execution-AQE"><a href="#5-Adaptive-Query-Execution-AQE" class="headerlink" title="5. Adaptive Query Execution (AQE)"></a>5. Adaptive Query Execution (AQE)</h2><p>Sparkâ€™s query optimizer that:</p>
<ul>
<li>Dynamically coalesces shuffle partitions</li>
<li>Optimizes join strategies</li>
<li>Handles skewed joins automatically</li>
</ul>
<p>To disable (not recommended for most cases):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.conf.<span class="built_in">set</span>(<span class="string">&quot;spark.sql.adaptive.enabled&quot;</span>, <span class="string">&quot;false&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Key-Takeaways"><a href="#Key-Takeaways" class="headerlink" title="Key Takeaways"></a>Key Takeaways</h2><ol>
<li><strong>Data Management</strong>: Master HDFS read&#x2F;write patterns for your environment  </li>
<li><strong>API Choice</strong>: Use SQL DataFrames for most ETL, Pandas API for small datasets  </li>
<li><strong>Execution Flow</strong>: Understand how jobs&#x2F;stages map to your code  </li>
<li><strong>Optimization</strong>: Leverage AQE unless debugging specific issues</li>
</ol>
<p>PySparkâ€™s power comes from its layered architecture - understanding these fundamentals helps build efficient data pipelines that scale with your quantitative workloads.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/02/pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/02/pytorch/" class="post-title-link" itemprop="url">PyTorch - Deep Learning tricks in asset management</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-02 18:28:42" itemprop="dateCreated datePublished" datetime="2023-06-02T18:28:42+08:00">2023-06-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-04 10:14:54" itemprop="dateModified" datetime="2025-04-04T10:14:54+08:00">2025-04-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-GPU-Memory-Management"><a href="#1-GPU-Memory-Management" class="headerlink" title="1. GPU Memory Management"></a>1. GPU Memory Management</h2><h3 id="Monitoring-VRAM-Usage"><a href="#Monitoring-VRAM-Usage" class="headerlink" title="Monitoring VRAM Usage"></a>Monitoring VRAM Usage</h3><p>Track memory allocation during training:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Allocated: <span class="subst">&#123;torch.cuda.memory_allocated()/<span class="number">1e6</span>:<span class="number">.2</span>f&#125;</span>MB&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Validation-Loop-Optimization"><a href="#Validation-Loop-Optimization" class="headerlink" title="Validation Loop Optimization"></a>Validation Loop Optimization</h3><p>Avoid memory leaks in validation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Problematic approach (accumulates tensors)</span></span><br><span class="line">val_preds = []</span><br><span class="line"><span class="keyword">for</span> x_val <span class="keyword">in</span> val_dataloader:</span><br><span class="line">    val_preds.append(model(x_val))  <span class="comment"># Memory grows!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Better approach (store scalar losses)</span></span><br><span class="line">val_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> val_dataloader:</span><br><span class="line">    loss = loss_fn(model(x_val), y_val)</span><br><span class="line">    val_loss += loss.item()  <span class="comment"># No memory accumulation</span></span><br><span class="line">val_loss /= <span class="built_in">len</span>(val_dataloader)</span><br></pre></td></tr></table></figure>

<h2 id="2-Model-Inspection-with-Torchsummary"><a href="#2-Model-Inspection-with-Torchsummary" class="headerlink" title="2. Model Inspection with Torchsummary"></a>2. Model Inspection with Torchsummary</h2><p>Analyze model architecture for multi-input networks:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="comment"># For model with two input branches</span></span><br><span class="line">summary(model, [(<span class="number">20</span>,), (<span class="number">50</span>, <span class="number">10</span>)])  <span class="comment"># Input shapes without batch size</span></span><br></pre></td></tr></table></figure>

<h2 id="3-Memory-Impact-Breakdown"><a href="#3-Memory-Impact-Breakdown" class="headerlink" title="3. Memory Impact Breakdown"></a>3. Memory Impact Breakdown</h2><h3 id="Critical-Memory-Milestones"><a href="#Critical-Memory-Milestones" class="headerlink" title="Critical Memory Milestones"></a>Critical Memory Milestones</h3><ol>
<li><strong>Model Initialization</strong>:</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.to(device)  <span class="comment"># ~132KB for small models</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>Data Loading</strong>:</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Batch memory calculation</span></span><br><span class="line">batch_size = <span class="number">4096</span></span><br><span class="line">features = <span class="number">20</span></span><br><span class="line">bytes_per_float = <span class="number">4</span></span><br><span class="line">memory = batch_size * features * bytes_per_float  <span class="comment"># 328KB</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>Forward Pass</strong>:</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out = model(batch)  <span class="comment"># Adds ~6.8MB (includes computation graph)</span></span><br></pre></td></tr></table></figure>

<h2 id="4-Loss-Function-Engineering"><a href="#4-Loss-Function-Engineering" class="headerlink" title="4. Loss Function Engineering"></a>4. Loss Function Engineering</h2><h3 id="Scaling-Considerations"><a href="#Scaling-Considerations" class="headerlink" title="Scaling Considerations"></a>Scaling Considerations</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Original loss</span></span><br><span class="line">loss = F.mse_loss(pred, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Scaled loss (affects regularization)</span></span><br><span class="line">scaled_loss = loss * <span class="number">1000</span>  </span><br><span class="line">scaled_loss.backward()</span><br></pre></td></tr></table></figure>
<p><strong>Key Insight</strong>: Loss scaling effectively increases learning rate for Adam optimizers</p>
<h2 id="5-Data-Pipeline-Pitfalls"><a href="#5-Data-Pipeline-Pitfalls" class="headerlink" title="5. Data Pipeline Pitfalls"></a>5. Data Pipeline Pitfalls</h2><h3 id="Shuffle-Safeguard"><a href="#Shuffle-Safeguard" class="headerlink" title="Shuffle Safeguard"></a>Shuffle Safeguard</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Correct test loader setup</span></span><br><span class="line">test_loader = DataLoader(</span><br><span class="line">    dataset, </span><br><span class="line">    shuffle=<span class="literal">False</span>,  <span class="comment"># Crucial for time series!</span></span><br><span class="line">    batch_size=<span class="number">64</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Shuffling test data can destroy sequential relationships in financial data</p>
<h2 id="6-Custom-Layer-Implementation"><a href="#6-Custom-Layer-Implementation" class="headerlink" title="6. Custom Layer Implementation"></a>6. Custom Layer Implementation</h2><h3 id="Gramian-Angular-Field-GAF-Layer"><a href="#Gramian-Angular-Field-GAF-Layer" class="headerlink" title="Gramian Angular Field (GAF) Layer"></a>Gramian Angular Field (GAF) Layer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GAFLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.weights = nn.Parameter(torch.Tensor(input_dim, input_dim))</span><br><span class="line">        nn.init.kaiming_uniform_(<span class="variable language_">self</span>.weights)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        cos_matrix = torch.cos(torch.einsum(<span class="string">&#x27;bi,ij,bj-&gt;bij&#x27;</span>, x, <span class="variable language_">self</span>.weights, x))</span><br><span class="line">        <span class="keyword">return</span> cos_matrix</span><br></pre></td></tr></table></figure>

<h2 id="7-Layer-Initialization-Best-Practices"><a href="#7-Layer-Initialization-Best-Practices" class="headerlink" title="7. Layer Initialization Best Practices"></a>7. Layer Initialization Best Practices</h2><h3 id="Avoid-Parameter-Sharing"><a href="#Avoid-Parameter-Sharing" class="headerlink" title="Avoid Parameter Sharing"></a>Avoid Parameter Sharing</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Correct approach for parallel branches</span></span><br><span class="line"><span class="variable language_">self</span>.cnn1 = nn.Conv1d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line"><span class="variable language_">self</span>.cnn2 = nn.Conv1d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>)  <span class="comment"># Separate parameters</span></span><br><span class="line"><span class="variable language_">self</span>.cnn3 = nn.Conv1d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<h2 id="8-Input-Shape-Conventions"><a href="#8-Input-Shape-Conventions" class="headerlink" title="8. Input Shape Conventions"></a>8. Input Shape Conventions</h2><table>
<thead>
<tr>
<th>Layer Type</th>
<th>Expected Shape</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>CNN</td>
<td>(N, C, H, W)</td>
<td>Channels-first</td>
</tr>
<tr>
<td>RNN</td>
<td>(N, Seq_len, C)</td>
<td>Time-major</td>
</tr>
<tr>
<td>Transformer</td>
<td>(Seq_len, N, C)</td>
<td>Source&#x2F;target variants</td>
</tr>
</tbody></table>
<h2 id="9-Activation-Monitoring-with-Hooks"><a href="#9-Activation-Monitoring-with-Hooks" class="headerlink" title="9. Activation Monitoring with Hooks"></a>9. Activation Monitoring with Hooks</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">activation = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">register_hook</span>(<span class="params">name</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hook_fn</span>(<span class="params">module, <span class="built_in">input</span>, output</span>):</span><br><span class="line">        activation[name] = output.detach()</span><br><span class="line">    <span class="keyword">return</span> hook_fn</span><br><span class="line"></span><br><span class="line">model.conv1.register_forward_hook(register_hook(<span class="string">&#x27;conv1_output&#x27;</span>))</span><br></pre></td></tr></table></figure>

<h2 id="10-Tensor-Operations-Pro-Tips"><a href="#10-Tensor-Operations-Pro-Tips" class="headerlink" title="10. Tensor Operations Pro Tips"></a>10. Tensor Operations Pro Tips</h2><h3 id="Transposition-Nuances"><a href="#Transposition-Nuances" class="headerlink" title="Transposition Nuances"></a>Transposition Nuances</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">x.t()   <span class="comment"># Error: only works for 2D</span></span><br><span class="line">x.T     <span class="comment"># Transposes all dimensions</span></span><br><span class="line">x.mT    <span class="comment"># Matrix transpose (last 2 dims)</span></span><br></pre></td></tr></table></figure>

<h3 id="Efficient-Outer-Product"><a href="#Efficient-Outer-Product" class="headerlink" title="Efficient Outer Product"></a>Efficient Outer Product</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Using einsum for pairwise interactions</span></span><br><span class="line">outer = torch.einsum(<span class="string">&#x27;ncx,ncy-&gt;ncxy&#x27;</span>, x, x)</span><br></pre></td></tr></table></figure>

<h2 id="Key-Takeaways"><a href="#Key-Takeaways" class="headerlink" title="Key Takeaways"></a>Key Takeaways</h2><ol>
<li><strong>Memory Awareness</strong>: Profile VRAM usage at critical stages  </li>
<li><strong>Custom Components</strong>: Build domain-specific layers while respecting initialization rules  </li>
<li><strong>Data Sanity</strong>: Preserve temporal relationships in financial data  </li>
<li><strong>Inspection Tools</strong>: Leverage hooks and summary for model debugging</li>
</ol>
<p>PyTorchâ€™s flexibility enables powerful modeling for quantitative applications - these patterns will help avoid common pitfalls while maintaining computational efficiency.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/12/numba/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/12/numba/" class="post-title-link" itemprop="url">Numba - Performance with Pitfalls</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-04-12 20:11:02" itemprop="dateCreated datePublished" datetime="2023-04-12T20:11:02+08:00">2023-04-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-04 10:15:09" itemprop="dateModified" datetime="2025-04-04T10:15:09+08:00">2025-04-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Numba is a powerful JIT compiler for Python, but it comes with quirks. Below are key lessons learned from hands-on experience.</p>
<hr>
<h2 id="1-1-Binary-Literal-Comparisons"><a href="#1-1-Binary-Literal-Comparisons" class="headerlink" title="1.1 Binary Literal Comparisons"></a>1.1 Binary Literal Comparisons</h2><p>Direct byte string comparisons in Numba can be treacherous. While checking individual bytes works (<code>side[0] == 66</code>), full string comparisons (<code>side == b&quot;x&quot;</code>) may fail silently. </p>
<p><strong>Solution:</strong> Implement element-wise comparison:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bytes_equal</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(a) != <span class="built_in">len</span>(b): <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a)):</span><br><span class="line">        <span class="keyword">if</span> a[i] != b[i]: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<h2 id="1-2-NaN-and-Infinity-Handling"><a href="#1-2-NaN-and-Infinity-Handling" class="headerlink" title="1.2 NaN and Infinity Handling"></a>1.2 NaN and Infinity Handling</h2><p>Numba doesnâ€™t automatically handle NaN&#x2F;Inf in reductions. Implement custom safe operations:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nan_safe_sum</span>(<span class="params">arr</span>):</span><br><span class="line">    total = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> val <span class="keyword">in</span> arr:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.isnan(val):</span><br><span class="line">            total += val</span><br><span class="line">    <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure>


<h2 id="1-3-Type-Initialization-Matters"><a href="#1-3-Type-Initialization-Matters" class="headerlink" title="1.3 Type Initialization Matters"></a>1.3 Type Initialization Matters</h2><p>Improper initialization leads to silent type coercion:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dangerous</span>():</span><br><span class="line">    l = [np.nan, np.nan]  <span class="comment"># Infers float type</span></span><br><span class="line">    l = [<span class="number">0</span>, <span class="number">0</span>]            <span class="comment"># OK - remains float</span></span><br><span class="line">    l = [<span class="number">1</span>, <span class="number">2.5</span>]          <span class="comment"># OK </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Wrong approach:</span></span><br><span class="line">    l = [<span class="number">0</span>, <span class="number">0</span>]            <span class="comment"># Infers int type</span></span><br><span class="line">    l[<span class="number">0</span>] = <span class="number">2.5</span>            <span class="comment"># Silent failure!</span></span><br></pre></td></tr></table></figure>


<h2 id="1-4-Index-Errors-Silent-Corruption"><a href="#1-4-Index-Errors-Silent-Corruption" class="headerlink" title="1.4 Index Errors: Silent Corruption"></a>1.4 Index Errors: Silent Corruption</h2><p>Out-of-bounds accesses return garbage values instead of errors:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dangerous_access</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="comment"># May return corrupted data instead of error:</span></span><br><span class="line">    <span class="built_in">print</span>(data[<span class="string">&quot;time&quot;</span>][<span class="built_in">len</span>(data) + <span class="number">100</span>])  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Protection:</span></span><br><span class="line">    <span class="keyword">assert</span> index &lt; <span class="built_in">len</span>(data), <span class="string">&quot;Out of bounds&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="1-5-Loop-Optimization-Limits"><a href="#1-5-Loop-Optimization-Limits" class="headerlink" title="1.5 Loop Optimization Limits"></a>1.5 Loop Optimization Limits</h2><p>Numba may not optimize nested loops effectively. Test shows 10x slowdown when adding layers:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">slow_nested</span>(<span class="params">x</span>):  <span class="comment"># 18s vs 2s for single loop</span></span><br><span class="line">    out = np.zeros(<span class="number">7</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">2</span>]):</span><br><span class="line">                out[i] += x[i,j,k] * <span class="number">0.1</span></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<h2 id="1-6-Set-Mutation-Danger"><a href="#1-6-Set-Mutation-Danger" class="headerlink" title="1.6 Set Mutation Danger"></a>1.6 Set Mutation Danger</h2><p>Modifying sets during iteration causes undefined behavior:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dangerous_set</span>():</span><br><span class="line">    s = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> s:</span><br><span class="line">        s.add(item*<span class="number">2</span>)  <span class="comment"># May hang or corrupt data</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="1-7-Optimizing-Multi-Layer-Loops"><a href="#1-7-Optimizing-Multi-Layer-Loops" class="headerlink" title="1.7 Optimizing Multi-Layer Loops"></a>1.7 Optimizing Multi-Layer Loops</h2><p>Focus optimization efforts on innermost loops. For regression calculations:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fast_regression</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># Optimize this innermost loop:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="comment"># Vectorized operations here</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>



<h2 id="1-8-Common-Errors"><a href="#1-8-Common-Errors" class="headerlink" title="1.8 Common Errors"></a>1.8 Common Errors</h2><p>A. Silent NaN Corruption</p>
<p>All-NaN arrays can trigger unexpected NumPy errors</p>
<p>B. Kernel Deaths<br>Futures cancelled errors often indicate memory issues or infinite loops</p>
<p>C. Cross-Asset Errors</p>
<p>Data type mismatches across different securities:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Problematic:</span></span><br><span class="line">prio = np.int32(security_data)  <span class="comment"># Fails for large values</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Solution:</span></span><br><span class="line">prio = np.arange(<span class="built_in">len</span>(security_data))  <span class="comment"># Use counter</span></span><br></pre></td></tr></table></figure>
<p>Pro Tip: Liberal use of assert statements helps catch edge cases early in Numba-compiled code.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">safe_function</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="keyword">assert</span> data.dtype == np.float64, <span class="string">&quot;Requires float64&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="keyword">not</span> np.isnan(data).<span class="built_in">any</span>(), <span class="string">&quot;NaNs detected&quot;</span></span><br></pre></td></tr></table></figure>



      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/25/linux-remote/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/08/25/linux-remote/" class="post-title-link" itemprop="url">Linux Remote Development</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-08-25 18:04:54" itemprop="dateCreated datePublished" datetime="2022-08-25T18:04:54+08:00">2022-08-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-04 10:13:41" itemprop="dateModified" datetime="2025-04-04T10:13:41+08:00">2025-04-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Linux-Remote-Development-Tricks-and-Workflows"><a href="#Linux-Remote-Development-Tricks-and-Workflows" class="headerlink" title="Linux Remote Development: Tricks and Workflows"></a>Linux Remote Development: Tricks and Workflows</h1><h2 id="1-SSH-Tunnel-Mastery"><a href="#1-SSH-Tunnel-Mastery" class="headerlink" title="1. SSH Tunnel Mastery"></a>1. SSH Tunnel Mastery</h2><h3 id="Debugging-Connections"><a href="#Debugging-Connections" class="headerlink" title="Debugging Connections"></a>Debugging Connections</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -vv user@remote_host  <span class="comment"># Verbose debugging output</span></span><br></pre></td></tr></table></figure>

<h3 id="1-1-Local-Port-Forwarding"><a href="#1-1-Local-Port-Forwarding" class="headerlink" title="1.1 Local Port Forwarding"></a>1.1 Local Port Forwarding</h3><p>Access blocked resources through intermediate server:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -L 8181:target_host:3389 user@jump_host  <span class="comment"># Tunnel through jump host</span></span><br></pre></td></tr></table></figure>

<h3 id="1-2-Dynamic-SOCKS-Proxy"><a href="#1-2-Dynamic-SOCKS-Proxy" class="headerlink" title="1.2 Dynamic SOCKS Proxy"></a>1.2 Dynamic SOCKS Proxy</h3><p>Bypass network restrictions:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -D 8181 user@proxy_host  <span class="comment"># Creates SOCKS5 proxy on local port 8181</span></span><br></pre></td></tr></table></figure>

<h3 id="1-3-Reverse-Tunneling-Magic"><a href="#1-3-Reverse-Tunneling-Magic" class="headerlink" title="1.3 Reverse Tunneling Magic"></a>1.3 Reverse Tunneling Magic</h3><p>Expose local services behind NAT:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run on local machine behind firewall</span></span><br><span class="line">ssh -R 10022:localhost:22 user@public_server  <span class="comment"># Expose SSH via public server</span></span><br></pre></td></tr></table></figure>

<h3 id="1-4-Auto-maintained-Reverse-Tunnel"><a href="#1-4-Auto-maintained-Reverse-Tunnel" class="headerlink" title="1.4 Auto-maintained Reverse Tunnel"></a>1.4 Auto-maintained Reverse Tunnel</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autossh -M 5000 -fNTR 10022:localhost:22 user@cloud_server</span><br></pre></td></tr></table></figure>
<p><strong>Pro Tip</strong>: Use <code>lsof -i -P -n | grep LISTEN</code> to troubleshoot port conflicts</p>
<h2 id="2-Multi-hop-SSH-Gateway"><a href="#2-Multi-hop-SSH-Gateway" class="headerlink" title="2. Multi-hop SSH Gateway"></a>2. Multi-hop SSH Gateway</h2><h3 id="Smart-ssh-config-Setup"><a href="#Smart-ssh-config-Setup" class="headerlink" title="Smart ~&#x2F;.ssh&#x2F;config Setup"></a>Smart ~&#x2F;.ssh&#x2F;config Setup</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Host cloud_gateway</span><br><span class="line">    HostName andycai.top</span><br><span class="line">    User ubuntu</span><br><span class="line">    IdentityFile ~/.ssh/cloud_key</span><br><span class="line"></span><br><span class="line">Host company_raspi</span><br><span class="line">    HostName localhost</span><br><span class="line">    Port 10022</span><br><span class="line">    ProxyCommand ssh -W %h:%p cloud_gateway</span><br><span class="line"></span><br><span class="line">Host internal_server</span><br><span class="line">    HostName inno-srv41</span><br><span class="line">    User cjt</span><br><span class="line">    ProxyCommand ssh -W %h:%p company_raspi</span><br></pre></td></tr></table></figure>
<p>Access internal servers directly with:<br><code>ssh internal_server</code></p>
<h2 id="3-Terminal-Multiplexing-with-TMUX"><a href="#3-Terminal-Multiplexing-with-TMUX" class="headerlink" title="3. Terminal Multiplexing with TMUX"></a>3. Terminal Multiplexing with TMUX</h2><h3 id="Essential-Shortcuts"><a href="#Essential-Shortcuts" class="headerlink" title="Essential Shortcuts"></a>Essential Shortcuts</h3><ul>
<li><strong>Split Panes</strong><br><code>Ctrl-b %</code> Vertical | <code>Ctrl-b &quot;</code> Horizontal</li>
<li><strong>Session Management</strong><br><code>Ctrl-b d</code> Detach | <code>tmux attach</code> Reconnect</li>
<li><strong>Window Control</strong><br><code>Ctrl-b c</code> New window | <code>Ctrl-b n/p</code> Cycle windows</li>
</ul>
<h3 id="Fix-Layout-Issues"><a href="#Fix-Layout-Issues" class="headerlink" title="Fix Layout Issues"></a>Fix Layout Issues</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux detach-client -a  <span class="comment"># Reset misbehaving sessions</span></span><br></pre></td></tr></table></figure>

<h2 id="4-Systemd-Service-Management"><a href="#4-Systemd-Service-Management" class="headerlink" title="4. Systemd Service Management"></a>4. Systemd Service Management</h2><h3 id="Create-Persistent-Tunnel-Service"><a href="#Create-Persistent-Tunnel-Service" class="headerlink" title="Create Persistent Tunnel Service"></a>Create Persistent Tunnel Service</h3><ol>
<li>Create service file:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nano /etc/systemd/system/piTunnel.service</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>Service definition:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Persistent Reverse SSH Tunnel</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=pi</span><br><span class="line">ExecStart=/usr/bin/autossh -M 0 -NTR 10022:localhost:22 cloud_server</span><br><span class="line">Restart=always</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>Enable and monitor:</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl daemon-reload</span><br><span class="line"><span class="built_in">sudo</span> systemctl <span class="built_in">enable</span> piTunnel</span><br><span class="line">systemctl status piTunnel  <span class="comment"># Verify operation</span></span><br></pre></td></tr></table></figure>

<h3 id="Startup-Optimization"><a href="#Startup-Optimization" class="headerlink" title="Startup Optimization"></a>Startup Optimization</h3><p>Generate dependency graph:  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemd-analyze plot &gt; boot_analysis.svg</span><br></pre></td></tr></table></figure>

<h2 id="5-Pro-Tips-for-Production"><a href="#5-Pro-Tips-for-Production" class="headerlink" title="5. Pro Tips for Production"></a>5. Pro Tips for Production</h2><ol>
<li><p><strong>Connection Keepalives</strong><br>Add to ~&#x2F;.ssh&#x2F;config:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ServerAliveInterval 60</span><br><span class="line">TCPKeepAlive <span class="built_in">yes</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Persistent Tmux Sessions</strong>  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tmux new -s dev_session  <span class="comment"># Create named session</span></span><br><span class="line">tmux attach -t dev_session  <span class="comment"># Reattach after disconnect</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>SSH Config Inheritance</strong><br>Use <code>Include</code> for modular configuration:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Include ~/.ssh/conf.d/*  <span class="comment"># Split config into multiple files</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Key-Takeaways"><a href="#Key-Takeaways" class="headerlink" title="Key Takeaways"></a>Key Takeaways</h2><ol>
<li><strong>Tunnel Strategically</strong>: Choose -L&#x2F;-R&#x2F;-D based on network topology  </li>
<li><strong>Automate Everything</strong>: Systemd services &gt; manual processes  </li>
<li><strong>Terminal Resilience</strong>: TMUX sessions survive disconnections  </li>
<li><strong>Config as Code</strong>: Maintain SSH connections through config files</li>
</ol>
<p>These techniques form the backbone of professional remote development workflows - master them to work seamlessly across distributed environments.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/31/gibbs-sampling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Andy Cai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Efficient Corner">
      <meta itemprop="description" content="A quantitative portfolio manager passionate about dissecting complexity at the intersection of finance, math, and technology. This site is my digital labâ€”a space where I document technical deep dives, code snippets, and analytical explorations into algorithmic trading, risk modeling, data science, and the occasional offbeat problem that sparks my curiosity.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Efficient Corner">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/31/gibbs-sampling/" class="post-title-link" itemprop="url">Gibbs Sampling</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-10-31 21:49:55" itemprop="dateCreated datePublished" datetime="2020-10-31T21:49:55+08:00">2020-10-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-04-04 10:15:41" itemprop="dateModified" datetime="2025-04-04T10:15:41+08:00">2025-04-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>In many inference tasks, it is necessary to integrate out variables. However, this may not always be possible as the integral may not be an analytically trackable function. Thankfully, we can leverage the law of large numbers to estimate these integrals from sampling. This means if we want to estimate $\phi &#x3D; \int \phi(x) p(x) dx$, we can simply sample from the joint distribution $p(x)$. We can take the empirical average $ \frac{1}{N} \sum_{i&#x3D;1}^N \phi(x_i)$, for large enough N this would converge to the actual mean. The problem is that the form of $ p(x) $ may be undesirable and sampling can be made difficult. One method to deal with this problem is using Gibbs sampling, which samples from the conditional distribution instead.</p>
<h2 id="Gibbs-Sampling-method"><a href="#Gibbs-Sampling-method" class="headerlink" title="Gibbs Sampling method"></a>Gibbs Sampling method</h2><p>For each component i of x in turn, sample a new value from the conditional distribution of $x_i$ given all other $x_{j\neq i}$:  </p>
<p>$$\begin{align}<br>x_i &amp; \sim p(x_i|x1,â€¦,x_{i-1},x{i+1},â€¦,x_D)<br>\end{align} $$</p>
<p>This eventually generates dependent samples from the joint distribution $p(x)$.</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>In this notebook I implement a Gibbs sampler, for both 2D and 3D distributions. This has taken inspiration from the materials in the Cambridge University Probablistic Machine Learning Course.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tools <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> scipy.linalg <span class="keyword">import</span> sqrtm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<h2 id="Function-to-visualise-a-Guassian-with-a-given-variance"><a href="#Function-to-visualise-a-Guassian-with-a-given-variance" class="headerlink" title="Function to visualise a Guassian with a given variance"></a>Function to visualise a Guassian with a given variance</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_gaussian</span>(<span class="params">cov,mu,n,ax</span>):</span><br><span class="line">    mu = np.reshape(a=mu, newshape=(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">    theta = np.divide(<span class="built_in">range</span>(n),(n-<span class="number">1</span>))*<span class="number">2</span>*np.pi</span><br><span class="line">    epoints = np.matmul(sqrtm(cov),[np.cos(theta),np.sin(theta)])</span><br><span class="line">    epoints += np.matmul(mu, np.ones((<span class="number">1</span>,n)))</span><br><span class="line">    ax.plot(epoints[<span class="number">0</span>,:],epoints[<span class="number">1</span>,:])</span><br><span class="line">    ax.scatter(mu[<span class="number">0</span>,:],mu[<span class="number">1</span>,:],c =<span class="string">&#x27;r&#x27;</span>, s=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> </span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">plot_gaussian([[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>]],[<span class="number">1</span>,<span class="number">1</span>],<span class="number">60</span>, ax= ax)</span><br></pre></td></tr></table></figure>
<p><img data-src="/images/gibbs_1.png" title="Gaussian function"></p>
<h2 id="The-Gibbs-sampler-to-sample-from-the-2D-Gaussian"><a href="#The-Gibbs-sampler-to-sample-from-the-2D-Gaussian" class="headerlink" title="The Gibbs sampler to sample from the 2D Gaussian"></a>The Gibbs sampler to sample from the 2D Gaussian</h2><p>This function first plots the joint Gaussian distribution on the random walk axes, using the previously defined function. This allows the target distribution to be visualised.</p>
<p>The gibbs2 function then iterates throught the number of iterations require. For each iteration, it computes the conditional distribution of x given y, and vice versa. These give $p(x|y)$ and $p(y|x)$ to sample from. The conditionals are obtained by segmenting the covariance matrix in to sub matrices</p>
<p>\begin{align}<br>p(x,y) &amp; &#x3D; \mathcal{N}\left( \begin{bmatrix} a \ b \end{bmatrix}, \begin{bmatrix} A &amp; B \ B^T &amp; C \end{bmatrix}\right) \<br>&amp; \to \<br>p(x|y) &amp; &#x3D; \mathcal{N}(a+BC^{-1}(y-b), A-BC^{-1}B^T)<br>\end{align}</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gibbs2</span>(<span class="params">N, ax1,ax2,cov=[[<span class="number">1</span>, <span class="number">0.5</span>],[<span class="number">0.5</span>,<span class="number">1</span>]], m =[<span class="number">0</span>,<span class="number">0</span>]</span>):</span><br><span class="line">    cov = np.array(cov)</span><br><span class="line">    ci = np.linalg.inv(cov)</span><br><span class="line">    <span class="comment"># init x at -2,2 </span></span><br><span class="line">    x = -<span class="number">2</span></span><br><span class="line">    y = <span class="number">2</span></span><br><span class="line">    plot_gaussian(cov=<span class="number">4</span>*cov, ax=ax1,mu=m,n=<span class="number">300</span>)</span><br><span class="line">    xx=[x]</span><br><span class="line">    yy=[y]</span><br><span class="line"></span><br><span class="line">    diff_list =[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        xold = x</span><br><span class="line">        yold = y</span><br><span class="line">        x= -ci[<span class="number">0</span>,<span class="number">1</span>]*y/ci[<span class="number">0</span>,<span class="number">0</span>] + np.random.randn()/np.sqrt(ci[<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">        <span class="comment">#print([xold,x])</span></span><br><span class="line">        ax1.plot([xold,x],[yold,y])</span><br><span class="line">        xold = x;</span><br><span class="line">        y = -ci[<span class="number">0</span>,<span class="number">1</span>]*x/ci[<span class="number">1</span>,<span class="number">1</span>]+ np.random.randn()/np.sqrt(ci[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(xx)&gt;<span class="number">1</span>:</span><br><span class="line">            emp_cov = np.cov(np.array(xx),np.array(yy))</span><br><span class="line">            diff = -<span class="built_in">abs</span>(cov-emp_cov).<span class="built_in">sum</span>()</span><br><span class="line">            diff_list.append(diff)</span><br><span class="line">            ax2.clear()</span><br><span class="line">            ax2.plot(diff_list)</span><br><span class="line">            ax2.set_title(<span class="string">&#x27;Convergence plot&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            ax2.set_xlabel(<span class="string">&#x27;iterations&#x27;</span>)</span><br><span class="line">            ax2.set_ylabel(<span class="string">&#x27;Empirical Covariance Matrix Error&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        ax1.plot([xold,x],[yold,y])</span><br><span class="line">        ax1.scatter(x,y, s= <span class="number">30</span>)</span><br><span class="line">        xx.append(x)</span><br><span class="line">        yy.append(y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Uncomment this section and set %matplotlib to none to see live plot</span></span><br><span class="line">        <span class="comment">#plt.draw()</span></span><br><span class="line">        <span class="comment">#if i ==0:</span></span><br><span class="line">            <span class="comment">#plt.pause(0.1)</span></span><br><span class="line">        <span class="comment">#plt.pause(0.005)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">ax1= plt.subplot(<span class="number">211</span>)</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Gibbs Sampling process on a 2D Gaussian distribution and Convergence plot&#x27;</span>,fontsize=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">ax1.set_xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;x2&#x27;</span>)</span><br><span class="line">ax1.set_title(<span class="string">&#x27;Markov Chain Random Walk&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax2= plt.subplot(<span class="number">212</span>)</span><br><span class="line">ax2.set_title(<span class="string">&#x27;Convergence plot&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax2.set_xlabel(<span class="string">&#x27;iterations&#x27;</span>)</span><br><span class="line"><span class="comment">#plt.ion()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#ani = animation.FuncAnimation(figs,animate, interval=1000)</span></span><br><span class="line"><span class="comment">#plt.show()</span></span><br><span class="line">gibbs2(<span class="number">100</span>,ax1,ax2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img data-src="/images/gibbs_2.png"></p>
<h2 id="Function-to-sample-from-the-3-dimensional-Gaussian"><a href="#Function-to-sample-from-the-3-dimensional-Gaussian" class="headerlink" title="Function to sample from the 3 dimensional Gaussian"></a>Function to sample from the 3 dimensional Gaussian</h2><p>For the trivariate Guassian, the conditionals are more tricky to compute, as in the 2D case the conditional probability equations boil down to a set of scalars, where as in this case we see that matrices $A,B,C$ are matrices. We use a for loop to generate these matrices when we look at each of the variables [x1,x2,x3].</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gibbs3</span>(<span class="params">N, ax1,ax,cov=[[<span class="number">1</span>, <span class="number">0.5</span>,<span class="number">0.3</span>],[<span class="number">0.5</span>,<span class="number">1</span>,<span class="number">0.2</span>],[<span class="number">0.3</span>,<span class="number">0.2</span>,<span class="number">1</span>]], mu =[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>]</span>):</span><br><span class="line">    cov = np.array(cov)</span><br><span class="line">    <span class="comment"># init x at -2,2 </span></span><br><span class="line">    x = np.array([-<span class="number">2.0</span>,<span class="number">2.0</span>,<span class="number">2.0</span>])</span><br><span class="line">    mu = np.array(mu)</span><br><span class="line">    xx=np.array([[],[],[]])</span><br><span class="line">    diff_list=[]</span><br><span class="line">    xold=[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>]</span><br><span class="line">    xold[<span class="number">0</span>]= x[<span class="number">0</span>]</span><br><span class="line">    xold[<span class="number">1</span>]=x[<span class="number">1</span>]</span><br><span class="line">    xold[<span class="number">2</span>]=x[<span class="number">2</span>]</span><br><span class="line">    ite=[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            ind_list = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">            ind_list.remove(j)</span><br><span class="line">            B=np.array([cov[j,ind_list[<span class="number">0</span>]],cov[j,ind_list[<span class="number">1</span>]]])</span><br><span class="line">            C=np.array([[cov[ind_list[<span class="number">0</span>],ind_list[<span class="number">0</span>]],cov[ind_list[<span class="number">0</span>],ind_list[<span class="number">1</span>]]],[cov[ind_list[<span class="number">1</span>],ind_list[<span class="number">0</span>]],cov[ind_list[<span class="number">1</span>],ind_list[<span class="number">1</span>]]]])</span><br><span class="line">            Ci = np.linalg.inv(C)</span><br><span class="line">            D = np.array(np.subtract([x[k] <span class="keyword">for</span> k <span class="keyword">in</span> ind_list],[mu[k] <span class="keyword">for</span> k <span class="keyword">in</span> ind_list]))</span><br><span class="line">            mean= mu[j]+np.matmul(np.matmul(B,Ci),D) </span><br><span class="line">            dev = np.sqrt(cov[j,j]-np.matmul(np.matmul(B,Ci),B.T))</span><br><span class="line">            x[j] =<span class="built_in">float</span>(mean)+np.random.randn()/<span class="built_in">float</span>(dev)</span><br><span class="line"></span><br><span class="line">            ax1.plot([xold[<span class="number">0</span>],x[<span class="number">0</span>]],[xold[<span class="number">1</span>],x[<span class="number">1</span>]],[xold[<span class="number">2</span>],x[<span class="number">2</span>]],alpha = <span class="number">0.3</span>)</span><br><span class="line">            ax1.scatter(*x,s=<span class="number">20</span>,alpha=<span class="number">0.4</span>)</span><br><span class="line">            xx=np.concatenate((xx,np.reshape(x.T,(<span class="number">3</span>,<span class="number">1</span>))), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            xold[j]=x[j]</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(xx[<span class="number">0</span>])&gt;<span class="number">3</span> <span class="keyword">and</span> <span class="built_in">len</span>(xx[<span class="number">0</span>])%<span class="number">8</span>==<span class="number">1</span>:</span><br><span class="line">                emp_cov = np.cov(np.array(xx))</span><br><span class="line">                diff = <span class="built_in">abs</span>(cov-emp_cov).<span class="built_in">sum</span>()</span><br><span class="line">                diff_list.append(diff)</span><br><span class="line">                ax.clear()</span><br><span class="line">                ax.plot(ite,diff_list)</span><br><span class="line">                ax.set_title(<span class="string">&#x27;Convergence Plot&#x27;</span>)</span><br><span class="line">                ax.set_ylabel(<span class="string">&#x27;Covariance matrix absolute error&#x27;</span>)</span><br><span class="line">                ax.set_xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">                ite.append(ite[-<span class="number">1</span>]+<span class="number">8</span>)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">14</span>,<span class="number">5</span>))</span><br><span class="line">ax1= plt.subplot(<span class="number">121</span>,projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">ax1.set_title(<span class="string">&#x27;Markov Chain Monte Carlo Random Walk on trivariate Gaussian&#x27;</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;x2&#x27;</span>)</span><br><span class="line">ax1.set_zlabel(<span class="string">&#x27;x3&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax=plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Convergence Plot&#x27;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&#x27;Covariance matrix absolute error&#x27;</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">plt.ion()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#ani = animation.FuncAnimation(figs,animate, interval=1000)</span></span><br><span class="line"><span class="comment">#plt.show()</span></span><br><span class="line">gibbs3(<span class="number">300</span>,ax1,ax)</span><br></pre></td></tr></table></figure>

<p><img data-src="/images/gibbs_3.png"></p>
<h2 id="Interpretation-of-results-and-conclusion"><a href="#Interpretation-of-results-and-conclusion" class="headerlink" title="Interpretation of results and conclusion"></a>Interpretation of results and conclusion</h2><p>The gibbs sampler was able to successfully sample from the 2D and 3D distributions. It was observed that for the 2D case, the algorithm very rapidly converged in under 50 iterations. This means the variance of the sampled data was able to rapidly become similar to the expected distribution. However, for the 3D case, very slow convergence behaviour has been observed. The reason for this is currently unclear, and could be a consequence of how the convergence is defined.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Andy Cai</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/ajc327" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
